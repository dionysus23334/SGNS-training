{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "803047a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found data/text8, skip.\n"
     ]
    }
   ],
   "source": [
    "# download_text8.py\n",
    "import os, zipfile, urllib.request\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "url = \"http://mattmahoney.net/dc/text8.zip\"\n",
    "zip_path = \"data/text8.zip\"\n",
    "txt_path = \"data/text8\"\n",
    "\n",
    "if not os.path.exists(txt_path):\n",
    "    print(\"Downloading text8.zip ...\")\n",
    "    urllib.request.urlretrieve(url, zip_path)\n",
    "    print(\"Unzipping ...\")\n",
    "    with zipfile.ZipFile(zip_path) as zf:\n",
    "        zf.extractall(\"data\")\n",
    "    print(\"OK → data/text8\")\n",
    "else:\n",
    "    print(\"Found data/text8, skip.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5697de3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SGNS on Text8 ... (vocab pruning: min_count=5)\n",
      "✅ Done. Training time: 1.1 min with workers=31\n",
      "Saved to outputs/w2v_text8_sgns.model / .vec\n",
      "\n",
      "Top-10 for 'king':\n",
      "  queen           0.5844\n",
      "  canute          0.5761\n",
      "  kings           0.5650\n",
      "  haakon          0.5608\n",
      "  vasa            0.5554\n",
      "  valdemar        0.5469\n",
      "  montferrat      0.5410\n",
      "  corvinus        0.5396\n",
      "  nemanja         0.5373\n",
      "  valois          0.5367\n",
      "\n",
      "Top-10 for 'queen':\n",
      "  elizabeth       0.6709\n",
      "  boleyn          0.6055\n",
      "  margrethe       0.6046\n",
      "  highness        0.6018\n",
      "  regnant         0.5978\n",
      "  consort         0.5896\n",
      "  hrh             0.5893\n",
      "  king            0.5844\n",
      "  isabella        0.5720\n",
      "  monarch         0.5659\n",
      "\n",
      "Top-10 for 'london':\n",
      "  kensington      0.6006\n",
      "  manchester      0.5840\n",
      "  newham          0.5738\n",
      "  guildhall       0.5723\n",
      "  piccadilly      0.5708\n",
      "  southwark       0.5645\n",
      "  surrey          0.5574\n",
      "  hertfordshire   0.5573\n",
      "  paddington      0.5563\n",
      "  holborn         0.5532\n",
      "\n",
      "Top-10 for 'computer':\n",
      "  computers       0.7234\n",
      "  minicomputer    0.6152\n",
      "  bootstrap       0.5997\n",
      "  microcomputer   0.5945\n",
      "  hardware        0.5942\n",
      "  hci             0.5865\n",
      "  interfacing     0.5843\n",
      "  asic            0.5841\n",
      "  computing       0.5835\n",
      "  touchscreen     0.5830\n"
     ]
    }
   ],
   "source": [
    "# train_text8_gensim.py\n",
    "import os, time, multiprocessing as mp\n",
    "from pathlib import Path\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "TEXT8 = Path(\"data/text8\")\n",
    "assert TEXT8.exists(), \"缺少 data/text8，请先运行 download_text8.py\"\n",
    "\n",
    "def read_text8(path: Path, chunk=10_000):\n",
    "    # Text8 是一行空格分词文本；切成“伪句子”块更利于迭代\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        toks = f.read().strip().split()\n",
    "    n = len(toks)\n",
    "    for i in range(0, n, chunk):\n",
    "        yield toks[i:i+chunk]\n",
    "\n",
    "def main():\n",
    "    os.makedirs(\"outputs\", exist_ok=True)\n",
    "    sentences = list(read_text8(TEXT8))\n",
    "\n",
    "    # 推荐超参（SGNS）\n",
    "    vector_size = 300   # 词向量维度\n",
    "    window      = 5     # 窗口大小\n",
    "    negative    = 10    # 负采样个数\n",
    "    sample      = 1e-3  # 子采样阈值\n",
    "    min_count   = 5\n",
    "    epochs      = 5\n",
    "    workers     = max(1, mp.cpu_count() - 1)\n",
    "\n",
    "    print(f\"Training SGNS on Text8 ... (vocab pruning: min_count={min_count})\")\n",
    "    t0 = time.time()\n",
    "    model = Word2Vec(\n",
    "        sentences=sentences,\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        sg=1,                 # 1=Skip-Gram\n",
    "        negative=negative,\n",
    "        sample=sample,\n",
    "        min_count=min_count,\n",
    "        workers=workers,\n",
    "        epochs=epochs,\n",
    "        seed=42\n",
    "    )\n",
    "    dt = time.time() - t0\n",
    "    print(f\"✅ Done. Training time: {dt/60:.1f} min with workers={workers}\")\n",
    "\n",
    "    # 保存\n",
    "    model.save(\"outputs/w2v_text8_sgns.model\")\n",
    "    model.wv.save_word2vec_format(\"outputs/w2v_text8_sgns.vec\", binary=False)\n",
    "    print(\"Saved to outputs/w2v_text8_sgns.model / .vec\")\n",
    "\n",
    "    # 快速查看几个词的相似词\n",
    "    probe = [\"king\", \"queen\", \"london\", \"computer\"]\n",
    "    for q in probe:\n",
    "        if q in model.wv:\n",
    "            print(f\"\\nTop-10 for '{q}':\")\n",
    "            for w, s in model.wv.most_similar(q, topn=10):\n",
    "                print(f\"  {w:15s} {s:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "w2v_sgns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
