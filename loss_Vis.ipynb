{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeca75a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] alpha=0.5 | ns_exponent=0.5 | epochs=10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 84\u001b[0m\n\u001b[0;32m     76\u001b[0m model \u001b[38;5;241m=\u001b[39m Word2Vec(\n\u001b[0;32m     77\u001b[0m     sg\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, vector_size\u001b[38;5;241m=\u001b[39mVECTOR_SIZE, window\u001b[38;5;241m=\u001b[39mWINDOW,\n\u001b[0;32m     78\u001b[0m     negative\u001b[38;5;241m=\u001b[39mNEGATIVE, ns_exponent\u001b[38;5;241m=\u001b[39malpha,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     81\u001b[0m     compute_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     82\u001b[0m )\n\u001b[0;32m     83\u001b[0m model\u001b[38;5;241m.\u001b[39mbuild_vocab(SENTS)\n\u001b[1;32m---> 84\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mSENTS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorpus_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mloss_cb\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m dt \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[0;32m     89\u001b[0m loss_curves[alpha] \u001b[38;5;241m=\u001b[39m loss_cb\u001b[38;5;241m.\u001b[39mlosses\n",
      "File \u001b[1;32mc:\\Users\\m1830\\.conda\\envs\\w2v_sgns\\lib\\site-packages\\gensim\\models\\word2vec.py:1073\u001b[0m, in \u001b[0;36mWord2Vec.train\u001b[1;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1070\u001b[0m     callback\u001b[38;5;241m.\u001b[39mon_epoch_begin(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1072\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m corpus_iterable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1073\u001b[0m     trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_epoch(\n\u001b[0;32m   1074\u001b[0m         corpus_iterable, cur_epoch\u001b[38;5;241m=\u001b[39mcur_epoch, total_examples\u001b[38;5;241m=\u001b[39mtotal_examples,\n\u001b[0;32m   1075\u001b[0m         total_words\u001b[38;5;241m=\u001b[39mtotal_words, queue_factor\u001b[38;5;241m=\u001b[39mqueue_factor, report_delay\u001b[38;5;241m=\u001b[39mreport_delay,\n\u001b[0;32m   1076\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1077\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1078\u001b[0m     trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_epoch_corpusfile(\n\u001b[0;32m   1079\u001b[0m         corpus_file, cur_epoch\u001b[38;5;241m=\u001b[39mcur_epoch, total_examples\u001b[38;5;241m=\u001b[39mtotal_examples, total_words\u001b[38;5;241m=\u001b[39mtotal_words,\n\u001b[0;32m   1080\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\m1830\\.conda\\envs\\w2v_sgns\\lib\\site-packages\\gensim\\models\\word2vec.py:1434\u001b[0m, in \u001b[0;36mWord2Vec._train_epoch\u001b[1;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay, callbacks)\u001b[0m\n\u001b[0;32m   1431\u001b[0m     thread\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# make interrupting the process with ctrl+c easier\u001b[39;00m\n\u001b[0;32m   1432\u001b[0m     thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m-> 1434\u001b[0m trained_word_count, raw_word_count, job_tally \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_epoch_progress\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1435\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprogress_queue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_queue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_examples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1436\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreport_delay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport_delay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_corpus_file_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1437\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1439\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trained_word_count, raw_word_count, job_tally\n",
      "File \u001b[1;32mc:\\Users\\m1830\\.conda\\envs\\w2v_sgns\\lib\\site-packages\\gensim\\models\\word2vec.py:1289\u001b[0m, in \u001b[0;36mWord2Vec._log_epoch_progress\u001b[1;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[0;32m   1286\u001b[0m unfinished_worker_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers\n\u001b[0;32m   1288\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m unfinished_worker_count \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1289\u001b[0m     report \u001b[38;5;241m=\u001b[39m \u001b[43mprogress_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# blocks if workers too slow\u001b[39;00m\n\u001b[0;32m   1290\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m report \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# a thread reporting that it finished\u001b[39;00m\n\u001b[0;32m   1291\u001b[0m         unfinished_worker_count \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\m1830\\.conda\\envs\\w2v_sgns\\lib\\queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[1;32m--> 171\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a non-negative number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\m1830\\.conda\\envs\\w2v_sgns\\lib\\threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ===== alpha_sweep_w2v_text8_with_loss_and_overlay.py =====\n",
    "import os, time, re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "# --------- 路径与超参 ---------\n",
    "TEXT8 = Path(\"data/text8\")\n",
    "assert TEXT8.exists(), \"缺少 data/text8，请先准备语料（http://mattmahoney.net/dc/textdata.html）。\"\n",
    "\n",
    "OUT = Path(\"outputs\"); OUT.mkdir(exist_ok=True)\n",
    "ALPHAS = [0.5, 0.6, 0.75, 0.9, 1.0]   # ← 5 个 α\n",
    "VECTOR_SIZE = 300\n",
    "WINDOW = 5\n",
    "NEGATIVE = 10\n",
    "SAMPLE = 1e-3\n",
    "MIN_COUNT = 5\n",
    "EPOCHS = 10\n",
    "SEED = 42\n",
    "WORKERS = (os.cpu_count() and max(1, os.cpu_count()-1)) or 1\n",
    "\n",
    "# --------- 数据读取：把 text8 切成“伪句子”块 ---------\n",
    "def read_text8(path: Path, chunk=10_000):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        toks = f.read().strip().split()\n",
    "    for i in range(0, len(toks), chunk):\n",
    "        yield toks[i:i+chunk]\n",
    "\n",
    "SENTS = list(read_text8(TEXT8))  # 列表化，便于多次遍历\n",
    "\n",
    "# --------- 训练损失回调：逐 epoch 记录“增量损失” ---------\n",
    "class EpochLossLogger(CallbackAny2Vec):\n",
    "    def __init__(self):\n",
    "        self.losses = []\n",
    "        self.prev = 0.0\n",
    "    def on_epoch_end(self, model):\n",
    "        loss_cum = model.get_latest_training_loss()\n",
    "        self.losses.append(loss_cum - self.prev)  # 本 epoch 的增量\n",
    "        self.prev = loss_cum\n",
    "\n",
    "# --------- 可视化所需工具：子集选择 + PCA ---------\n",
    "def collect_subset(wv, seed_words, nn_per_seed=12, limit=300):\n",
    "    chosen, seen = [], set()\n",
    "    seeds = [w for w in seed_words if w in wv]\n",
    "    for s in seeds:\n",
    "        if s not in seen:\n",
    "            chosen.append(s); seen.add(s)\n",
    "        for w, _ in wv.most_similar(s, topn=nn_per_seed):\n",
    "            if w not in seen:\n",
    "                chosen.append(w); seen.add(w)\n",
    "            if len(chosen) >= limit: break\n",
    "        if len(chosen) >= limit: break\n",
    "    if len(chosen) < limit:\n",
    "        for w in wv.index_to_key:\n",
    "            if w not in seen:\n",
    "                chosen.append(w); seen.add(w)\n",
    "            if len(chosen) >= limit: break\n",
    "    return chosen\n",
    "\n",
    "def pca_reduce(X, n_components=2):\n",
    "    Xc = X - X.mean(axis=0, keepdims=True)\n",
    "    U, S, Vt = np.linalg.svd(Xc, full_matrices=False)\n",
    "    return Xc @ Vt[:n_components].T\n",
    "\n",
    "# --------- 主流程：训练 5 个 α，记录损失并保存向量 ---------\n",
    "loss_curves = {}         # alpha -> [loss_epoch1, ..., loss_epochE]\n",
    "models_kv = {}           # alpha -> KeyedVectors\n",
    "vec_paths = {}           # alpha -> path\n",
    "\n",
    "for alpha in ALPHAS:\n",
    "    print(f\"[train] alpha={alpha} | ns_exponent={alpha} | epochs={EPOCHS}\")\n",
    "    loss_cb = EpochLossLogger()\n",
    "    t0 = time.time()\n",
    "    model = Word2Vec(\n",
    "        sg=1, vector_size=VECTOR_SIZE, window=WINDOW,\n",
    "        negative=NEGATIVE, ns_exponent=alpha,\n",
    "        sample=SAMPLE, min_count=MIN_COUNT,\n",
    "        workers=WORKERS, seed=SEED,\n",
    "        compute_loss=True\n",
    "    )\n",
    "    model.build_vocab(SENTS)\n",
    "    model.train(\n",
    "        SENTS, total_examples=model.corpus_count, epochs=EPOCHS,\n",
    "        callbacks=[loss_cb]\n",
    "    )\n",
    "    dt = time.time() - t0\n",
    "    loss_curves[alpha] = loss_cb.losses\n",
    "    print(f\"  → done in {dt/60:.1f} min | losses per epoch: {[round(x,2) for x in loss_cb.losses]}\")\n",
    "    # 保存\n",
    "    model_path = OUT / f\"w2v_text8_alpha{alpha:.3f}.model\"\n",
    "    vec_path   = OUT / f\"w2v_text8_alpha{alpha:.3f}.vec\"\n",
    "    # model.save(str(model_path))\n",
    "    # model.wv.save_word2vec_format(str(vec_path), binary=False)\n",
    "    models_kv[alpha] = model.wv\n",
    "    vec_paths[alpha] = str(vec_path)\n",
    "\n",
    "# --------- 图1：不同 α 的训练损失曲线 ---------\n",
    "plt.figure(figsize=(10,6))\n",
    "for alpha in ALPHAS:\n",
    "    y = loss_curves[alpha]\n",
    "    x = list(range(1, len(y)+1))\n",
    "    plt.plot(x, y, marker='o', linewidth=1, label=f\"α={alpha}\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Training loss increment per epoch (↓)\")\n",
    "plt.title(\"SGNS on Text8 — Loss vs Epoch for different α\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend()\n",
    "out_loss = OUT / \"loss_vs_epoch_by_alpha.png\"\n",
    "plt.tight_layout(); plt.savefig(out_loss, dpi=200); plt.close()\n",
    "print(f\"✅ Saved loss figure → {out_loss}\")\n",
    "\n",
    "# --------- 图2/图3：五种 α 的 2D/3D 叠绘（联合 PCA，坐标一致）---------\n",
    "# 选一批词（基于中间一个 α 的近邻，例如 0.75；若不存在就用第一个）\n",
    "alpha_ref = 0.75 if 0.75 in models_kv else ALPHAS[0]\n",
    "wv_ref = models_kv[alpha_ref]\n",
    "seed_words = [\"king\",\"queen\",\"man\",\"woman\",\"london\",\"paris\",\"france\",\"england\",\n",
    "              \"computer\",\"software\",\"data\",\"science\",\"music\",\"art\",\"city\",\"country\",\"river\",\"mountain\"]\n",
    "WORDS = collect_subset(wv_ref, seed_words, nn_per_seed=12, limit=300)\n",
    "\n",
    "# 联合 PCA：把 5 组 [len(WORDS) x D] 拼起来做一次 PCA\n",
    "blocks = []\n",
    "for alpha in ALPHAS:\n",
    "    wv = models_kv[alpha]\n",
    "    X = np.stack([wv[w] if w in wv else np.zeros(VECTOR_SIZE, dtype=np.float32) for w in WORDS], axis=0)\n",
    "    blocks.append(X)\n",
    "X_all = np.vstack(blocks)                               # [5*W, D]\n",
    "Z2_all = pca_reduce(X_all, n_components=2)              # [5*W, 2]\n",
    "Z3_all = pca_reduce(X_all, n_components=3)              # [5*W, 3]\n",
    "\n",
    "# 按块切回每个 α\n",
    "W = len(WORDS)\n",
    "coords2 = {alpha: Z2_all[i*W:(i+1)*W] for i, alpha in enumerate(ALPHAS)}\n",
    "coords3 = {alpha: Z3_all[i*W:(i+1)*W] for i, alpha in enumerate(ALPHAS)}\n",
    "\n",
    "# 2D 叠绘\n",
    "plt.figure(figsize=(12,9))\n",
    "for alpha in ALPHAS:\n",
    "    Z = coords2[alpha]\n",
    "    plt.scatter(Z[:,0], Z[:,1], s=12, label=f\"α={alpha}\", alpha=0.8)\n",
    "# 只标注部分词（种子），避免过密\n",
    "for w in seed_words:\n",
    "    if w in WORDS:\n",
    "        j = WORDS.index(w)\n",
    "        # 用参考 α 的坐标标注，防止多次重叠\n",
    "        z = coords2[alpha_ref][j]\n",
    "        plt.annotate(w, (z[0], z[1]), fontsize=8, alpha=0.9)\n",
    "plt.title(\"Word Embeddings (PCA 2D) — overlay of 5 α\")\n",
    "plt.tight_layout()\n",
    "out_2d = OUT / \"emb_compare_2d_5alpha.png\"\n",
    "plt.legend()\n",
    "plt.savefig(out_2d, dpi=200); plt.close()\n",
    "print(f\"✅ Saved 2D overlay → {out_2d}\")\n",
    "\n",
    "# 3D 叠绘\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa\n",
    "fig = plt.figure(figsize=(12,9))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "for alpha in ALPHAS:\n",
    "    Z = coords3[alpha]\n",
    "    ax.scatter(Z[:,0], Z[:,1], Z[:,2], s=12, label=f\"α={alpha}\", alpha=0.85)\n",
    "# 参考 α 的种子词标注\n",
    "for w in seed_words:\n",
    "    if w in WORDS:\n",
    "        j = WORDS.index(w)\n",
    "        z = coords3[alpha_ref][j]\n",
    "        ax.text(z[0], z[1], z[2], w, fontsize=7)\n",
    "ax.set_title(\"Word Embeddings (PCA 3D) — overlay of 5 α\")\n",
    "plt.tight_layout()\n",
    "out_3d = OUT / \"emb_compare_3d_5alpha.png\"\n",
    "plt.legend()\n",
    "plt.savefig(out_3d, dpi=200); plt.close()\n",
    "print(f\"✅ Saved 3D overlay → {out_3d}\")\n",
    "\n",
    "print(\"\\nAll done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d8e2ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] alpha=0.5 | epochs=10\n",
      "  -> time 4.2 min | loss per epoch: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[train] alpha=0.6 | epochs=10\n",
      "  -> time 4.3 min | loss per epoch: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[train] alpha=0.75 | epochs=10\n",
      "  -> time 4.2 min | loss per epoch: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[train] alpha=0.9 | epochs=10\n",
      "  -> time 4.4 min | loss per epoch: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[train] alpha=1.0 | epochs=10\n",
      "  -> time 5.0 min | loss per epoch: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "✅ Saved → outputs/loss_vs_epoch_by_alpha.png\n",
      "✅ Saved → outputs/grad_vw_vs_epoch.png\n",
      "✅ Saved → outputs/grad_uc_vs_epoch.png\n",
      "✅ Saved → outputs/grad_un_vs_epoch.png\n",
      "✅ Saved → outputs/emb_compare_2d_5alpha.png\n",
      "✅ Saved → outputs/emb_compare_3d_5alpha.png\n",
      "\n",
      "All done.\n"
     ]
    }
   ],
   "source": [
    "# ===== alpha_sweep_w2v_text8_with_loss_and_grads.py =====\n",
    "import os, time, math, re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "# ---------------- 路径与超参 ----------------\n",
    "TEXT8 = Path(\"data/text8\")\n",
    "assert TEXT8.exists(), \"缺少 data/text8，请先下载放到 data/text8\"\n",
    "\n",
    "OUT = Path(\"outputs\"); OUT.mkdir(exist_ok=True)\n",
    "ALPHAS = [0.5, 0.6, 0.75, 0.9, 1.0]   # 5个 α\n",
    "VECTOR_SIZE = 300\n",
    "WINDOW = 5\n",
    "NEGATIVE = 10\n",
    "SAMPLE = 1e-3\n",
    "MIN_COUNT = 5\n",
    "EPOCHS = 10\n",
    "SEED = 42\n",
    "WORKERS = (os.cpu_count() and max(1, os.cpu_count()-1)) or 1\n",
    "\n",
    "# 梯度探针采样规模（适当即可，过大计算会慢）\n",
    "GRAD_PROBE_SAMPLES = 1000    # 每个 epoch 采样多少正样本对 (w,c)\n",
    "GRAD_PROBE_NEG_K   = NEGATIVE\n",
    "\n",
    "# ---------------- 读取 text8 → “伪句子” ----------------\n",
    "def read_text8(path: Path, chunk=10_000):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        toks = f.read().strip().split()\n",
    "    for i in range(0, len(toks), chunk):\n",
    "        yield toks[i:i+chunk]\n",
    "\n",
    "SENTS = list(read_text8(TEXT8))\n",
    "\n",
    "# ---------------- 简易工具 ----------------\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def sample_positive_pairs_indices(sents, word2id, window, n_samples, rng):\n",
    "    \"\"\"随机抽 n_samples 个 (w,c) 索引对，使用动态窗口；过滤 OOV。\"\"\"\n",
    "    pairs = []\n",
    "    V = len(word2id)\n",
    "    for _ in range(n_samples * 3):  # 多采一点尝试，直到凑够 n_samples\n",
    "        sent = sents[rng.integers(0, len(sents))]\n",
    "        if len(sent) < 3:\n",
    "            continue\n",
    "        i = rng.integers(0, len(sent))\n",
    "        w = sent[i]\n",
    "        if w not in word2id:\n",
    "            continue\n",
    "        r = int(rng.integers(1, window + 1))\n",
    "        left = max(0, i - r); right = min(len(sent), i + r + 1)\n",
    "        ctx_positions = [j for j in range(left, right) if j != i]\n",
    "        if not ctx_positions:\n",
    "            continue\n",
    "        j = ctx_positions[rng.integers(0, len(ctx_positions))]\n",
    "        c = sent[j]\n",
    "        if c not in word2id:\n",
    "            continue\n",
    "        pairs.append((word2id[w], word2id[c]))\n",
    "        if len(pairs) >= n_samples:\n",
    "            break\n",
    "    return pairs\n",
    "\n",
    "def build_neg_sampler_probs(model, alpha):\n",
    "    \"\"\"按 α 计算负采样分布 P_neg ∝ count^α （按词的索引顺序）。\"\"\"\n",
    "    wv = model.wv\n",
    "    counts = np.array([wv.get_vecattr(w, \"count\") for w in wv.index_to_key], dtype=np.float64)\n",
    "    probs = counts ** float(alpha)\n",
    "    probs_sum = probs.sum()\n",
    "    if probs_sum <= 0:\n",
    "        probs = np.ones_like(probs) / len(probs)\n",
    "    else:\n",
    "        probs /= probs_sum\n",
    "    return probs\n",
    "\n",
    "def probe_grad_norms(model, sents, alpha, window, n_samples=1000, K=10, seed=0):\n",
    "    \"\"\"\n",
    "    对当前模型参数做一次 SGNS 梯度探针：\n",
    "      返回 {'vw': mean ||dL/dv_w||, 'uc': mean ||dL/du_c||, 'un': mean ||dL/du_n||}\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    wv = model.wv\n",
    "    V = len(wv.index_to_key)\n",
    "    word2id = wv.key_to_index\n",
    "    V_in  = wv.vectors                         # [V, D]  输入向量 v_w\n",
    "    U_out = model.syn1neg                      # [V, D]  输出向量 u_*（负采样使用）\n",
    "    assert U_out is not None, \"model.syn1neg 缺失（确保 negative>0）\"\n",
    "\n",
    "    # 采样正样本对 (w,c)\n",
    "    pairs = sample_positive_pairs_indices(sents, word2id, window, n_samples, rng)\n",
    "    if not pairs:\n",
    "        return {'vw': None, 'uc': None, 'un': None}\n",
    "\n",
    "    # 负采样分布\n",
    "    p_neg = build_neg_sampler_probs(model, alpha)\n",
    "\n",
    "    vw_norms, uc_norms, un_norms = [], [], []\n",
    "\n",
    "    for (wi, ci) in pairs:\n",
    "        v_w = V_in[wi]             # (D,)\n",
    "        u_c = U_out[ci]            # (D,)\n",
    "        # 采 K 个负样本索引（允许重复，不排除 c，本探针近似即可）\n",
    "        neg_idx = rng.choice(len(p_neg), size=K, replace=True, p=p_neg)\n",
    "        u_neg = U_out[neg_idx]     # (K, D)\n",
    "\n",
    "        # 打分\n",
    "        s_pos = float(np.dot(u_c, v_w))\n",
    "        s_neg = u_neg @ v_w        # (K,)\n",
    "\n",
    "        # sigmoids\n",
    "        sp = sigmoid(s_pos)        # 标量\n",
    "        sn = sigmoid(s_neg)        # (K,)\n",
    "\n",
    "        # 梯度（见 SGNS 闭式公式）\n",
    "        # dL/dv_w = (σ(s_pos)-1)*u_c + Σ σ(s_neg_i)*u_neg_i\n",
    "        grad_vw = (sp - 1.0) * u_c + (sn.reshape(-1,1) * u_neg).sum(axis=0)\n",
    "        # dL/du_c = (σ(s_pos)-1)*v_w\n",
    "        grad_uc = (sp - 1.0) * v_w\n",
    "        # dL/du_{n_i} = σ(s_neg_i) * v_w\n",
    "        grad_un_each = (sn.reshape(-1,1) * v_w.reshape(1,-1))  # (K, D)\n",
    "\n",
    "        vw_norms.append(np.linalg.norm(grad_vw))\n",
    "        uc_norms.append(np.linalg.norm(grad_uc))\n",
    "        un_norms.append(np.linalg.norm(grad_un_each, axis=1).mean())  # 对 K 个负样本取均值\n",
    "\n",
    "    return {\n",
    "        'vw': float(np.mean(vw_norms)),\n",
    "        'uc': float(np.mean(uc_norms)),\n",
    "        'un': float(np.mean(un_norms)),\n",
    "    }\n",
    "\n",
    "# ---------------- 训练回调：记录损失 + 梯度探针 ----------------\n",
    "class LossAndGradLogger(CallbackAny2Vec):\n",
    "    def __init__(self, sents, alpha, window, n_samples=1000, K=10, seed=0):\n",
    "        self.prev_loss = 0.0\n",
    "        self.losses = []\n",
    "        self.grads_vw = []\n",
    "        self.grads_uc = []\n",
    "        self.grads_un = []\n",
    "        self.sents = sents\n",
    "        self.alpha = alpha\n",
    "        self.window = window\n",
    "        self.n_samples = n_samples\n",
    "        self.K = K\n",
    "        self.seed = seed\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        # 累计损失差分 → 本 epoch 增量\n",
    "        cur = model.get_latest_training_loss()\n",
    "        self.losses.append(cur - self.prev_loss)\n",
    "        self.prev_loss = cur\n",
    "        # 梯度探针\n",
    "        stats = probe_grad_norms(\n",
    "            model, self.sents, self.alpha, self.window,\n",
    "            n_samples=self.n_samples, K=self.K, seed=self.seed\n",
    "        )\n",
    "        self.grads_vw.append(stats['vw'])\n",
    "        self.grads_uc.append(stats['uc'])\n",
    "        self.grads_un.append(stats['un'])\n",
    "\n",
    "# ---------------- 可视化工具：子集+PCA ----------------\n",
    "def collect_subset(wv, seed_words, nn_per_seed=12, limit=300):\n",
    "    chosen, seen = [], set()\n",
    "    seeds = [w for w in seed_words if w in wv]\n",
    "    for s in seeds:\n",
    "        if s not in seen:\n",
    "            chosen.append(s); seen.add(s)\n",
    "        for w, _ in wv.most_similar(s, topn=nn_per_seed):\n",
    "            if w not in seen:\n",
    "                chosen.append(w); seen.add(w)\n",
    "            if len(chosen) >= limit: break\n",
    "        if len(chosen) >= limit: break\n",
    "    if len(chosen) < limit:\n",
    "        for w in wv.index_to_key:\n",
    "            if w not in seen:\n",
    "                chosen.append(w); seen.add(w)\n",
    "            if len(chosen) >= limit: break\n",
    "    return chosen\n",
    "\n",
    "def pca_reduce(X, n_components=2):\n",
    "    Xc = X - X.mean(axis=0, keepdims=True)\n",
    "    U, S, Vt = np.linalg.svd(Xc, full_matrices=False)\n",
    "    return Xc @ Vt[:n_components].T\n",
    "\n",
    "# ---------------- 主流程：训练五个 α，记录损失/梯度并保存模型 ----------------\n",
    "loss_curves = {}          # alpha -> [E]\n",
    "grad_curves = {}          # alpha -> dict{'vw':[E],'uc':[E],'un':[E]}\n",
    "models_kv = {}            # alpha -> KeyedVectors\n",
    "\n",
    "for alpha in ALPHAS:\n",
    "    print(f\"[train] alpha={alpha} | epochs={EPOCHS}\")\n",
    "    cb = LossAndGradLogger(\n",
    "        sents=SENTS, alpha=alpha, window=WINDOW,\n",
    "        n_samples=GRAD_PROBE_SAMPLES, K=GRAD_PROBE_NEG_K,\n",
    "        seed=SEED\n",
    "    )\n",
    "    t0 = time.time()\n",
    "    # 显式 build_vocab + train，才能在回调中看到逐 epoch 损失\n",
    "    model = Word2Vec(\n",
    "        sg=1, vector_size=VECTOR_SIZE, window=WINDOW,\n",
    "        negative=NEGATIVE, ns_exponent=alpha,\n",
    "        sample=SAMPLE, min_count=MIN_COUNT,\n",
    "        workers=WORKERS, seed=SEED,\n",
    "        compute_loss=True\n",
    "    )\n",
    "    model.build_vocab(SENTS)\n",
    "    model.train(\n",
    "        SENTS, total_examples=model.corpus_count, epochs=EPOCHS,\n",
    "        callbacks=[cb]\n",
    "    )\n",
    "    print(f\"  -> time {(time.time()-t0)/60:.1f} min | loss per epoch: {[round(x,2) for x in cb.losses]}\")\n",
    "    # 保存\n",
    "    # model.save(str(OUT / f\"w2v_text8_alpha{alpha:.3f}.model\"))\n",
    "    # model.wv.save_word2vec_format(str(OUT / f\"w2v_text8_alpha{alpha:.3f}.vec\"), binary=False)\n",
    "    models_kv[alpha] = model.wv\n",
    "    loss_curves[alpha] = cb.losses\n",
    "    grad_curves[alpha] = {'vw': cb.grads_vw, 'uc': cb.grads_uc, 'un': cb.grads_un}\n",
    "\n",
    "# ---------------- 图：损失曲线（不同 α） ----------------\n",
    "plt.figure(figsize=(10,6))\n",
    "for alpha in ALPHAS:\n",
    "    y = loss_curves[alpha]; x = range(1, len(y)+1)\n",
    "    plt.plot(x, y, marker='o', linewidth=1, label=f\"α={alpha}\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Training loss increment per epoch (↓)\")\n",
    "plt.title(\"SGNS on Text8 — Loss vs Epoch (different α)\")\n",
    "plt.grid(alpha=0.3); plt.legend()\n",
    "plt.tight_layout(); plt.savefig(OUT/\"loss_vs_epoch_by_alpha.png\", dpi=200); plt.close()\n",
    "print(\"✅ Saved → outputs/loss_vs_epoch_by_alpha.png\")\n",
    "\n",
    "# ---------------- 图：梯度范数曲线（不同 α） ----------------\n",
    "def plot_grad(key, ylabel, out_name):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    for alpha in ALPHAS:\n",
    "        y = grad_curves[alpha][key]; x = range(1, len(y)+1)\n",
    "        plt.plot(x, y, marker='o', linewidth=1, label=f\"α={alpha}\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(ylabel)\n",
    "    plt.title(f\"SGNS on Text8 — {ylabel} vs Epoch (different α)\")\n",
    "    plt.grid(alpha=0.3); plt.legend()\n",
    "    plt.tight_layout(); plt.savefig(OUT/out_name, dpi=200); plt.close()\n",
    "    print(f\"✅ Saved → outputs/{out_name}\")\n",
    "\n",
    "plot_grad('vw', \"‖∂L/∂v_w‖ (mean over samples) (↓)\", \"grad_vw_vs_epoch.png\")\n",
    "plot_grad('uc', \"‖∂L/∂u_c‖ (mean over samples) (↓)\", \"grad_uc_vs_epoch.png\")\n",
    "plot_grad('un', \"‖∂L/∂u_n‖ (mean over negatives) (↓)\", \"grad_un_vs_epoch.png\")\n",
    "\n",
    "# ---------------- 词向量可视化：同图叠绘（2D/3D） ----------------\n",
    "alpha_ref = 0.75 if 0.75 in models_kv else ALPHAS[0]\n",
    "wv_ref = models_kv[alpha_ref]\n",
    "seed_words = [\"king\",\"queen\",\"man\",\"woman\",\"london\",\"paris\",\"france\",\"england\",\n",
    "              \"computer\",\"software\",\"data\",\"science\",\"music\",\"art\",\"city\",\"country\",\"river\",\"mountain\"]\n",
    "WORDS = collect_subset(wv_ref, seed_words, nn_per_seed=12, limit=300)\n",
    "\n",
    "# 联合 PCA（保证坐标一致）\n",
    "blocks = []\n",
    "for alpha in ALPHAS:\n",
    "    wv = models_kv[alpha]\n",
    "    X = np.stack([wv[w] if w in wv else np.zeros(VECTOR_SIZE, dtype=np.float32) for w in WORDS], axis=0)\n",
    "    blocks.append(X)\n",
    "X_all = np.vstack(blocks)\n",
    "Z2_all = pca_reduce(X_all, n_components=2)\n",
    "Z3_all = pca_reduce(X_all, n_components=3)\n",
    "\n",
    "W = len(WORDS)\n",
    "coords2 = {alpha: Z2_all[i*W:(i+1)*W] for i, alpha in enumerate(ALPHAS)}\n",
    "coords3 = {alpha: Z3_all[i*W:(i+1)*W] for i, alpha in enumerate(ALPHAS)}\n",
    "\n",
    "# 2D overlay\n",
    "plt.figure(figsize=(12,9))\n",
    "for alpha in ALPHAS:\n",
    "    Z = coords2[alpha]\n",
    "    plt.scatter(Z[:,0], Z[:,1], s=12, label=f\"α={alpha}\", alpha=0.8)\n",
    "# 只标注种子词（参考 α 的坐标），避免过密\n",
    "for w in seed_words:\n",
    "    if w in WORDS:\n",
    "        j = WORDS.index(w)\n",
    "        z = coords2[alpha_ref][j]\n",
    "        plt.annotate(w, (z[0], z[1]), fontsize=8, alpha=0.9)\n",
    "plt.title(\"Word Embeddings (PCA 2D) — overlay of 5 α\")\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig(OUT/\"emb_compare_2d_5alpha.png\", dpi=200); plt.close()\n",
    "print(\"✅ Saved → outputs/emb_compare_2d_5alpha.png\")\n",
    "\n",
    "# 3D overlay\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa\n",
    "fig = plt.figure(figsize=(12,9))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "for alpha in ALPHAS:\n",
    "    Z = coords3[alpha]\n",
    "    ax.scatter(Z[:,0], Z[:,1], Z[:,2], s=12, label=f\"α={alpha}\", alpha=0.85)\n",
    "for w in seed_words:\n",
    "    if w in WORDS:\n",
    "        j = WORDS.index(w)\n",
    "        z = coords3[alpha_ref][j]\n",
    "        ax.text(z[0], z[1], z[2], w, fontsize=7)\n",
    "ax.set_title(\"Word Embeddings (PCA 3D) — overlay of 5 α\")\n",
    "plt.tight_layout(); plt.legend()\n",
    "plt.savefig(OUT/\"emb_compare_3d_5alpha.png\", dpi=200); plt.close()\n",
    "print(\"✅ Saved → outputs/emb_compare_3d_5alpha.png\")\n",
    "\n",
    "print(\"\\nAll done.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "w2v_sgns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
