{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e5da4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets found: SimLex=Y WS353=Y Google=Y BATS=Y\n",
      "\n",
      "=== Evaluating: w2v_text8_alpha0.500.vec ===\n",
      "SimLex-999 Spearman: 0.3263 | coverage=99.30%\n",
      "WordSim-353 Spearman: 0.6574 | coverage=99.43%\n",
      "Google Analogies Acc: 27.58% | used=17827\n",
      "  - capital-common-countries  46.64% (n=506)\n",
      "  - capital-world         19.33% (n=3564)\n",
      "  - currency              14.93% (n=596)\n",
      "  - city-in-state         17.60% (n=2330)\n",
      "  - family                53.33% (n=420)\n",
      "BATS micro: 24.08% | macro: 14.87% | used=41970\n",
      "\n",
      "=== Evaluating: w2v_text8_alpha0.526.vec ===\n",
      "SimLex-999 Spearman: 0.3389 | coverage=99.30%\n",
      "WordSim-353 Spearman: 0.659 | coverage=99.43%\n",
      "Google Analogies Acc: 27.82% | used=17827\n",
      "  - capital-common-countries  48.81% (n=506)\n",
      "  - capital-world         19.92% (n=3564)\n",
      "  - currency              12.92% (n=596)\n",
      "  - city-in-state         17.21% (n=2330)\n",
      "  - family                53.81% (n=420)\n",
      "BATS micro: 24.80% | macro: 15.27% | used=41970\n",
      "\n",
      "=== Evaluating: w2v_text8_alpha0.553.vec ===\n",
      "SimLex-999 Spearman: 0.3443 | coverage=99.30%\n",
      "WordSim-353 Spearman: 0.6716 | coverage=99.43%\n",
      "Google Analogies Acc: 27.62% | used=17827\n",
      "  - capital-common-countries  47.63% (n=506)\n",
      "  - capital-world         20.20% (n=3564)\n",
      "  - currency              15.44% (n=596)\n",
      "  - city-in-state         16.61% (n=2330)\n",
      "  - family                55.24% (n=420)\n",
      "BATS micro: 25.08% | macro: 15.59% | used=41970\n",
      "\n",
      "=== Evaluating: w2v_text8_alpha0.579.vec ===\n",
      "SimLex-999 Spearman: 0.3372 | coverage=99.30%\n",
      "WordSim-353 Spearman: 0.6715 | coverage=99.43%\n",
      "Google Analogies Acc: 28.18% | used=17827\n",
      "  - capital-common-countries  46.64% (n=506)\n",
      "  - capital-world         19.14% (n=3564)\n",
      "  - currency              15.77% (n=596)\n",
      "  - city-in-state         16.18% (n=2330)\n",
      "  - family                57.38% (n=420)\n",
      "BATS micro: 25.03% | macro: 15.49% | used=41970\n",
      "\n",
      "=== Evaluating: w2v_text8_alpha0.605.vec ===\n",
      "SimLex-999 Spearman: 0.3462 | coverage=99.30%\n",
      "WordSim-353 Spearman: 0.6664 | coverage=99.43%\n",
      "Google Analogies Acc: 27.91% | used=17827\n",
      "  - capital-common-countries  50.00% (n=506)\n",
      "  - capital-world         21.44% (n=3564)\n",
      "  - currency              14.77% (n=596)\n",
      "  - city-in-state         15.06% (n=2330)\n",
      "  - family                53.33% (n=420)\n",
      "BATS micro: 25.38% | macro: 15.85% | used=41970\n",
      "\n",
      "=== Evaluating: w2v_text8_alpha0.632.vec ===\n",
      "SimLex-999 Spearman: 0.3384 | coverage=99.30%\n",
      "WordSim-353 Spearman: 0.6779 | coverage=99.43%\n",
      "Google Analogies Acc: 28.81% | used=17827\n",
      "  - capital-common-countries  53.95% (n=506)\n",
      "  - capital-world         22.31% (n=3564)\n",
      "  - currency              15.60% (n=596)\n",
      "  - city-in-state         16.22% (n=2330)\n",
      "  - family                54.52% (n=420)\n",
      "BATS micro: 25.55% | macro: 15.74% | used=41970\n",
      "\n",
      "=== Evaluating: w2v_text8_alpha0.658.vec ===\n",
      "SimLex-999 Spearman: 0.3492 | coverage=99.30%\n",
      "WordSim-353 Spearman: 0.685 | coverage=99.43%\n",
      "Google Analogies Acc: 28.27% | used=17827\n",
      "  - capital-common-countries  51.58% (n=506)\n",
      "  - capital-world         21.80% (n=3564)\n",
      "  - currency              14.60% (n=596)\n",
      "  - city-in-state         15.62% (n=2330)\n",
      "  - family                53.57% (n=420)\n",
      "BATS micro: 25.77% | macro: 16.00% | used=41970\n",
      "\n",
      "=== Evaluating: w2v_text8_alpha0.684.vec ===\n",
      "SimLex-999 Spearman: 0.3421 | coverage=99.30%\n",
      "WordSim-353 Spearman: 0.6818 | coverage=99.43%\n",
      "Google Analogies Acc: 29.15% | used=17827\n",
      "  - capital-common-countries  51.78% (n=506)\n",
      "  - capital-world         20.31% (n=3564)\n",
      "  - currency              15.44% (n=596)\n",
      "  - city-in-state         19.23% (n=2330)\n",
      "  - family                55.00% (n=420)\n",
      "BATS micro: 25.79% | macro: 16.05% | used=41970\n",
      "\n",
      "=== Evaluating: w2v_text8_alpha0.711.vec ===\n",
      "SimLex-999 Spearman: 0.3453 | coverage=99.30%\n",
      "WordSim-353 Spearman: 0.6722 | coverage=99.43%\n",
      "Google Analogies Acc: 28.83% | used=17827\n",
      "  - capital-common-countries  52.57% (n=506)\n",
      "  - capital-world         20.85% (n=3564)\n",
      "  - currency              15.77% (n=596)\n",
      "  - city-in-state         18.20% (n=2330)\n",
      "  - family                55.24% (n=420)\n",
      "BATS micro: 26.42% | macro: 16.31% | used=41970\n",
      "\n",
      "=== Evaluating: w2v_text8_alpha0.737.vec ===\n",
      "SimLex-999 Spearman: 0.345 | coverage=99.30%\n",
      "WordSim-353 Spearman: 0.6819 | coverage=99.43%\n",
      "Google Analogies Acc: 29.02% | used=17827\n",
      "  - capital-common-countries  49.80% (n=506)\n",
      "  - capital-world         22.17% (n=3564)\n",
      "  - currency              15.44% (n=596)\n",
      "  - city-in-state         18.45% (n=2330)\n",
      "  - family                55.00% (n=420)\n",
      "BATS micro: 26.50% | macro: 16.60% | used=41970\n",
      "\n",
      "=== Evaluating: w2v_text8_alpha0.763.vec ===\n",
      "SimLex-999 Spearman: 0.3396 | coverage=99.30%\n",
      "WordSim-353 Spearman: 0.6773 | coverage=99.43%\n",
      "Google Analogies Acc: 29.45% | used=17827\n",
      "  - capital-common-countries  53.95% (n=506)\n",
      "  - capital-world         23.12% (n=3564)\n",
      "  - currency              13.76% (n=596)\n",
      "  - city-in-state         17.47% (n=2330)\n",
      "  - family                51.67% (n=420)\n",
      "BATS micro: 26.68% | macro: 16.78% | used=41970\n",
      "\n",
      "=== Evaluating: w2v_text8_alpha0.789.vec ===\n",
      "SimLex-999 Spearman: 0.3517 | coverage=99.30%\n",
      "WordSim-353 Spearman: 0.683 | coverage=99.43%\n",
      "Google Analogies Acc: 30.18% | used=17827\n",
      "  - capital-common-countries  50.59% (n=506)\n",
      "  - capital-world         21.97% (n=3564)\n",
      "  - currency              14.77% (n=596)\n",
      "  - city-in-state         19.31% (n=2330)\n",
      "  - family                56.19% (n=420)\n",
      "BATS micro: 26.49% | macro: 16.44% | used=41970\n",
      "\n",
      "=== Evaluating: w2v_text8_alpha0.816.vec ===\n",
      "SimLex-999 Spearman: 0.3413 | coverage=99.30%\n",
      "WordSim-353 Spearman: 0.6819 | coverage=99.43%\n",
      "Google Analogies Acc: 29.80% | used=17827\n",
      "  - capital-common-countries  48.81% (n=506)\n",
      "  - capital-world         21.18% (n=3564)\n",
      "  - currency              16.78% (n=596)\n",
      "  - city-in-state         17.25% (n=2330)\n",
      "  - family                59.05% (n=420)\n",
      "BATS micro: 27.18% | macro: 17.36% | used=41970\n",
      "\n",
      "=== Evaluating: w2v_text8_alpha0.842.vec ===\n",
      "SimLex-999 Spearman: 0.3417 | coverage=99.30%\n",
      "WordSim-353 Spearman: 0.6844 | coverage=99.43%\n",
      "Google Analogies Acc: 29.72% | used=17827\n",
      "  - capital-common-countries  54.35% (n=506)\n",
      "  - capital-world         22.78% (n=3564)\n",
      "  - currency              13.93% (n=596)\n",
      "  - city-in-state         19.31% (n=2330)\n",
      "  - family                58.10% (n=420)\n",
      "BATS micro: 27.18% | macro: 17.04% | used=41970\n",
      "\n",
      "=== Evaluating: w2v_text8_alpha0.868.vec ===\n",
      "SimLex-999 Spearman: 0.3398 | coverage=99.30%\n",
      "WordSim-353 Spearman: 0.6874 | coverage=99.43%\n",
      "Google Analogies Acc: 30.26% | used=17827\n",
      "  - capital-common-countries  53.36% (n=506)\n",
      "  - capital-world         23.43% (n=3564)\n",
      "  - currency              16.44% (n=596)\n",
      "  - city-in-state         18.80% (n=2330)\n",
      "  - family                52.86% (n=420)\n",
      "BATS micro: 26.99% | macro: 16.77% | used=41970\n",
      "\n",
      "=== Evaluating: w2v_text8_alpha0.895.vec ===\n",
      "SimLex-999 Spearman: 0.334 | coverage=99.30%\n",
      "WordSim-353 Spearman: 0.686 | coverage=99.43%\n",
      "Google Analogies Acc: 29.70% | used=17827\n",
      "  - capital-common-countries  53.36% (n=506)\n",
      "  - capital-world         22.84% (n=3564)\n",
      "  - currency              12.75% (n=596)\n",
      "  - city-in-state         18.33% (n=2330)\n",
      "  - family                55.71% (n=420)\n",
      "BATS micro: 27.07% | macro: 16.81% | used=41970\n",
      "\n",
      "=== Evaluating: w2v_text8_alpha0.921.vec ===\n",
      "SimLex-999 Spearman: 0.3387 | coverage=99.30%\n",
      "WordSim-353 Spearman: 0.6881 | coverage=99.43%\n",
      "Google Analogies Acc: 29.42% | used=17827\n",
      "  - capital-common-countries  52.37% (n=506)\n",
      "  - capital-world         21.44% (n=3564)\n",
      "  - currency              17.28% (n=596)\n",
      "  - city-in-state         16.95% (n=2330)\n",
      "  - family                56.90% (n=420)\n",
      "BATS micro: 27.32% | macro: 17.38% | used=41970\n",
      "\n",
      "=== Evaluating: w2v_text8_alpha0.947.vec ===\n",
      "SimLex-999 Spearman: 0.3324 | coverage=99.30%\n",
      "WordSim-353 Spearman: 0.6813 | coverage=99.43%\n",
      "Google Analogies Acc: 29.42% | used=17827\n",
      "  - capital-common-countries  53.16% (n=506)\n",
      "  - capital-world         21.55% (n=3564)\n",
      "  - currency              13.59% (n=596)\n",
      "  - city-in-state         17.34% (n=2330)\n",
      "  - family                53.81% (n=420)\n",
      "BATS micro: 27.79% | macro: 17.46% | used=41970\n",
      "\n",
      "=== Evaluating: w2v_text8_alpha0.974.vec ===\n",
      "SimLex-999 Spearman: 0.344 | coverage=99.30%\n",
      "WordSim-353 Spearman: 0.6994 | coverage=99.43%\n",
      "Google Analogies Acc: 29.67% | used=17827\n",
      "  - capital-common-countries  55.73% (n=506)\n",
      "  - capital-world         22.53% (n=3564)\n",
      "  - currency              13.26% (n=596)\n",
      "  - city-in-state         16.27% (n=2330)\n",
      "  - family                57.62% (n=420)\n",
      "BATS micro: 27.71% | macro: 17.47% | used=41970\n",
      "\n",
      "=== Evaluating: w2v_text8_alpha1.000.vec ===\n",
      "SimLex-999 Spearman: 0.3382 | coverage=99.30%\n",
      "WordSim-353 Spearman: 0.6975 | coverage=99.43%\n",
      "Google Analogies Acc: 29.64% | used=17827\n",
      "  - capital-common-countries  56.13% (n=506)\n",
      "  - capital-world         22.81% (n=3564)\n",
      "  - currency              13.76% (n=596)\n",
      "  - city-in-state         16.70% (n=2330)\n",
      "  - family                52.14% (n=420)\n",
      "BATS micro: 27.48% | macro: 17.17% | used=41970\n",
      "\n",
      "=== Evaluating: w2v_text8_sgns.vec ===\n",
      "SimLex-999 Spearman: 0.3465 | coverage=99.30%\n",
      "WordSim-353 Spearman: 0.6807 | coverage=99.43%\n",
      "Google Analogies Acc: 29.04% | used=17827\n",
      "  - capital-common-countries  55.93% (n=506)\n",
      "  - capital-world         22.95% (n=3564)\n",
      "  - currency              15.10% (n=596)\n",
      "  - city-in-state         16.78% (n=2330)\n",
      "  - family                52.62% (n=420)\n",
      "BATS micro: 26.63% | macro: 16.63% | used=41970\n",
      "\n",
      "=== Evaluating: w2v_text8_sgns_abtt.vec ===\n",
      "SimLex-999 Spearman: 0.3132 | coverage=99.30%\n",
      "WordSim-353 Spearman: 0.6985 | coverage=99.43%\n",
      "Google Analogies Acc: 32.59% | used=17827\n",
      "  - capital-common-countries  62.25% (n=506)\n",
      "  - capital-world         26.68% (n=3564)\n",
      "  - currency              17.79% (n=596)\n",
      "  - city-in-state         19.40% (n=2330)\n",
      "  - family                57.38% (n=420)\n",
      "BATS micro: 29.79% | macro: 18.98% | used=41970\n",
      "\n",
      "✅ Done. Results saved to outputs/bench_results.json\n"
     ]
    }
   ],
   "source": [
    "# eval_authoritative.py\n",
    "# 评测权威基准：SimLex-999 / WordSim-353 / Google analogies / BATS\n",
    "# 依赖：numpy, gensim（无需 scipy）。Jupyter 友好（无 argparse）。\n",
    "\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# ----------------- 通用工具 -----------------\n",
    "def l2_normalize(mat, eps=1e-9):\n",
    "    nrm = np.linalg.norm(mat, axis=1, keepdims=True) + eps\n",
    "    return mat / nrm\n",
    "\n",
    "def _rankdata(a):\n",
    "    # 稳健的秩（考虑并列，取平均秩）\n",
    "    temp = np.argsort(a)\n",
    "    ranks = np.empty_like(temp, dtype=np.float64)\n",
    "    ranks[temp] = np.arange(len(a))\n",
    "    uniq, inv, cnt = np.unique(a, return_inverse=True, return_counts=True)\n",
    "    cum = np.cumsum(cnt)\n",
    "    start = cum - cnt\n",
    "    avg = (start + cum - 1) / 2.0\n",
    "    return avg[inv]\n",
    "\n",
    "def spearman_no_scipy(x, y):\n",
    "    rx, ry = _rankdata(x), _rankdata(y)\n",
    "    rx -= rx.mean(); ry -= ry.mean()\n",
    "    denom = (np.linalg.norm(rx) * np.linalg.norm(ry) + 1e-12)\n",
    "    return float(np.dot(rx, ry) / denom)\n",
    "\n",
    "# ----------------- 载入词向量 -----------------\n",
    "def load_vectors(vec_path, lowercase=True):\n",
    "    from gensim.models import KeyedVectors\n",
    "    wv = KeyedVectors.load_word2vec_format(vec_path, binary=False)\n",
    "    vocab = wv.index_to_key\n",
    "    if lowercase:\n",
    "        # 构建小写映射：若词表是小写的，这样可兼容大小写数据集\n",
    "        mapping = {}\n",
    "        for w in vocab:\n",
    "            wl = w.lower()\n",
    "            if wl not in mapping:  # 第一次出现的小写形式\n",
    "                mapping[wl] = w\n",
    "        return wv, mapping\n",
    "    else:\n",
    "        return wv, None\n",
    "\n",
    "def get_vec(wv, mapping, token, case_insensitive=True):\n",
    "    if not case_insensitive:\n",
    "        return (wv[token] if token in wv else None)\n",
    "    # CI：优先小写映射\n",
    "    wl = token.lower()\n",
    "    if wl in mapping:\n",
    "        return wv[mapping[wl]]\n",
    "    return (wv[token] if token in wv else None)\n",
    "\n",
    "# ----------------- 数据集：SimLex-999 -----------------\n",
    "def load_simlex(path):\n",
    "    \"\"\"\n",
    "    适配官方 SimLex-999 txt（tab分隔）格式：\n",
    "    word1, word2, POS, SimLex999, conc(w1), conc(w2), concQ, Assoc(USF), SimAssoc333, SD(SimLex)\n",
    "    返回: (pairs, gold) 或 None\n",
    "    \"\"\"\n",
    "    from pathlib import Path\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        return None\n",
    "\n",
    "    def norm(s):\n",
    "        return s.strip().lower().replace(\" \", \"\").replace(\"\\ufeff\", \"\")\n",
    "\n",
    "    pairs, scores = [], []\n",
    "    with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        header = f.readline()\n",
    "        if not header:\n",
    "            return None\n",
    "        # 主要是 \\t；容错逗号/空白\n",
    "        sep = \"\\t\" if \"\\t\" in header else (\",\" if \",\" in header else None)\n",
    "        def split(line):\n",
    "            if sep is None:\n",
    "                return [t for t in line.strip().replace(\"\\t\", \" \").split() if t]\n",
    "            else:\n",
    "                return [t.strip() for t in line.strip().split(sep)]\n",
    "\n",
    "        cols = [norm(c) for c in split(header)]\n",
    "        # 期望至少包含 word1、word2、simlex999\n",
    "        try:\n",
    "            i_w1 = cols.index(\"word1\")\n",
    "            i_w2 = cols.index(\"word2\")\n",
    "        except ValueError:\n",
    "            # 列名不对就直接放弃（上层会显示 SimLex=N）\n",
    "            return None\n",
    "        i_sc = cols.index(\"simlex999\") if \"simlex999\" in cols else None\n",
    "        if i_sc is None:\n",
    "            # 少数变体用 score\n",
    "            if \"score\" in cols:\n",
    "                i_sc = cols.index(\"score\")\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "        # 逐行读取\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            ps = split(line)\n",
    "            # 有些行尾可能多空列，确保够长\n",
    "            if len(ps) <= max(i_w1, i_w2, i_sc):\n",
    "                # 再尝试宽松按空白切分一次\n",
    "                ps = [t for t in line.strip().replace(\"\\t\", \" \").split() if t]\n",
    "                if len(ps) <= max(i_w1, i_w2, i_sc):\n",
    "                    continue\n",
    "            w1, w2 = ps[i_w1], ps[i_w2]\n",
    "            try:\n",
    "                s = float(ps[i_sc])\n",
    "            except Exception:\n",
    "                continue\n",
    "            if not w1 or not w2:\n",
    "                continue\n",
    "            pairs.append((w1, w2))\n",
    "            scores.append(s)\n",
    "\n",
    "    if not pairs:\n",
    "        return None\n",
    "    return pairs, np.array(scores, dtype=np.float64)\n",
    "\n",
    "def eval_similarity(wv, mapping, pairs, gold, case_insensitive=True):\n",
    "    # 计算余弦并与 gold 做 Spearman；返回 ρ 与覆盖率\n",
    "    vecs = []\n",
    "    used = []\n",
    "    for (a, b), s in zip(pairs, gold):\n",
    "        va = get_vec(wv, mapping, a, case_insensitive)\n",
    "        vb = get_vec(wv, mapping, b, case_insensitive)\n",
    "        if va is None or vb is None: \n",
    "            continue\n",
    "        # 归一化余弦（向量通常已近似零均值）\n",
    "        va_u = va / (np.linalg.norm(va) + 1e-12)\n",
    "        vb_u = vb / (np.linalg.norm(vb) + 1e-12)\n",
    "        vecs.append(np.dot(va_u, vb_u))\n",
    "        used.append(s)\n",
    "    if len(vecs) == 0:\n",
    "        return None, 0.0\n",
    "    rho = spearman_no_scipy(np.array(vecs), np.array(used, dtype=np.float64))\n",
    "    cov = len(vecs) / len(gold)\n",
    "    return rho, cov\n",
    "\n",
    "# ----------------- 数据集：WordSim-353 -----------------\n",
    "def load_wordsim(path):\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        return None\n",
    "    pairs, scores = [], []\n",
    "    with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        header = f.readline()\n",
    "        # 格式兼容：CSV（有表头）、空格/制表分隔、或 \"word1 word2 score\"\n",
    "        sep = \",\" if \",\" in header else None\n",
    "        def parse(line):\n",
    "            if sep is not None:\n",
    "                ps = [x.strip() for x in line.strip().split(\",\")]\n",
    "            else:\n",
    "                ps = [x.strip() for x in line.strip().replace(\"\\t\",\" \").split()]\n",
    "            if len(ps) < 3: return None\n",
    "            try:\n",
    "                return (ps[0], ps[1], float(ps[2]))\n",
    "            except:\n",
    "                return None\n",
    "        first = parse(header)\n",
    "        if first:\n",
    "            pairs.append((first[0], first[1])); scores.append(first[2])\n",
    "        for line in f:\n",
    "            it = parse(line)\n",
    "            if it:\n",
    "                pairs.append((it[0], it[1])); scores.append(it[2])\n",
    "    return pairs, np.array(scores, dtype=np.float64)\n",
    "\n",
    "# ----------------- 数据集：Google analogies -----------------\n",
    "def load_google_analogies(path):\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        return None\n",
    "    sections = []\n",
    "    current = []\n",
    "    current_name = \"ALL\"\n",
    "    with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line: \n",
    "                continue\n",
    "            if line.startswith(\":\"):\n",
    "                # 存上一个 section\n",
    "                if current:\n",
    "                    sections.append((current_name, current))\n",
    "                    current = []\n",
    "                current_name = line[1:].strip()\n",
    "            else:\n",
    "                parts = line.split()\n",
    "                if len(parts) == 4:\n",
    "                    current.append(tuple(parts))\n",
    "    if current:\n",
    "        sections.append((current_name, current))\n",
    "    return sections\n",
    "\n",
    "def eval_analogy(wv, mapping, sections, case_insensitive=True):\n",
    "    # 返回总准确率 + 分章节准确率\n",
    "    Xnorm_cache = {}  # 缓存归一化后的向量，减小重复计算\n",
    "    def vec(word):\n",
    "        if case_insensitive:\n",
    "            wl = word.lower()\n",
    "            if wl in Xnorm_cache: \n",
    "                return Xnorm_cache[wl]\n",
    "            v = get_vec(wv, mapping, word, case_insensitive)\n",
    "            if v is None: \n",
    "                Xnorm_cache[wl] = None\n",
    "                return None\n",
    "            vu = v / (np.linalg.norm(v) + 1e-12)\n",
    "            Xnorm_cache[wl] = vu\n",
    "            return vu\n",
    "        else:\n",
    "            if word in Xnorm_cache: \n",
    "                return Xnorm_cache[word]\n",
    "            if word not in wv:\n",
    "                Xnorm_cache[word] = None\n",
    "                return None\n",
    "            v = wv[word]; vu = v / (np.linalg.norm(v) + 1e-12)\n",
    "            Xnorm_cache[word] = vu\n",
    "            return vu\n",
    "\n",
    "    total = correct = 0\n",
    "    per_sec = []\n",
    "    vocab_mat = l2_normalize(wv.vectors)   # [|V|, D]\n",
    "    for name, items in sections:\n",
    "        t = c = 0\n",
    "        for a,b,c_,d in items:\n",
    "            va, vb, vc, vd = vec(a), vec(b), vec(c_), vec(d)\n",
    "            if va is None or vb is None or vc is None or vd is None:\n",
    "                continue\n",
    "            target = vb - va + vc\n",
    "            target /= (np.linalg.norm(target) + 1e-12)\n",
    "            scores = vocab_mat @ target\n",
    "            # 屏蔽 a, b, c 本身\n",
    "            for w in (a,b,c_):\n",
    "                idx = wv.key_to_index.get((mapping[w.lower()] if (case_insensitive and w.lower() in mapping) else w), None)\n",
    "                if idx is not None:\n",
    "                    scores[idx] = -np.inf\n",
    "            pred = int(np.argmax(scores))\n",
    "            t += 1\n",
    "            if np.allclose(vocab_mat[pred], vd, atol=0):  # 直接比较向量不可靠；改成索引比较\n",
    "                pass\n",
    "            # 比较词索引\n",
    "            didx = wv.key_to_index.get((mapping[d.lower()] if (case_insensitive and d.lower() in mapping) else d), None)\n",
    "            if didx is not None and pred == didx:\n",
    "                c += 1\n",
    "        total += t; correct += c\n",
    "        acc = (c / t) if t > 0 else None\n",
    "        per_sec.append((name, acc, t))\n",
    "    overall = (correct / total) if total > 0 else None\n",
    "    return overall, per_sec, total\n",
    "\n",
    "# ----------------- 数据集：BATS（目录） -----------------\n",
    "# ==== 放到 eval_authoritative.py，替换原来的 load_bats / 保留 eval_bats 或一起替换 ====\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def load_bats(root_dir, include_groups=None, max_pairs_per_file=None, random_state=42):\n",
    "    \"\"\"\n",
    "    读取 BATS_3.0：\n",
    "      - 支持“三种”格式：\n",
    "        1) 两列成对： album  albums\n",
    "        2) 4元组一行： a b c d\n",
    "        3) '::' 左右两侧成对集合： a b :: c d [c2 d2 ...]\n",
    "      - 每个文件视为一个任务；将文件内的所有 (a,b) 与 (c,d) 做笛卡尔积，生成 (a,b,c,d)\n",
    "      - include_groups: 仅包含目录前缀，如 ('1_','2_')\n",
    "      - max_pairs_per_file: 若文件对数过多，可随机下采样到该数量（避免 OOM）\n",
    "    返回: [(task_name, [(a,b,c,d), ...]), ...] 或 None\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    root = Path(root_dir)\n",
    "    if not root.exists() or not root.is_dir():\n",
    "        return None\n",
    "\n",
    "    # 收集要读的 .txt 文件\n",
    "    files = []\n",
    "    for sub in sorted(root.iterdir()):\n",
    "        if not sub.is_dir():\n",
    "            continue\n",
    "        if include_groups and not any(sub.name.startswith(p) for p in include_groups):\n",
    "            continue\n",
    "        files += sorted(sub.glob(\"*.txt\"))\n",
    "\n",
    "    tasks = []\n",
    "    for f in files:\n",
    "        name = f.relative_to(root).as_posix()\n",
    "        pairs = []         # 存 (a,b)\n",
    "        quads_direct = []  # 若文件直接给了 4 元组\n",
    "\n",
    "        with open(f, \"r\", encoding=\"utf-8\", errors=\"ignore\") as fh:\n",
    "            for line in fh:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "\n",
    "                # 格式3： '::' 左右成对集合\n",
    "                if \"::\" in line:\n",
    "                    L, R = line.split(\"::\", 1)\n",
    "                    ltoks = [t for t in re.split(r\"\\s+\", L.strip()) if t]\n",
    "                    rtoks = [t for t in re.split(r\"\\s+\", R.strip()) if t]\n",
    "                    lpairs = [(ltoks[i], ltoks[i+1]) for i in range(0, len(ltoks)-1, 2)]\n",
    "                    rpairs = [(rtoks[i], rtoks[i+1]) for i in range(0, len(rtoks)-1, 2)]\n",
    "                    for a,b in lpairs:\n",
    "                        for c,d in rpairs:\n",
    "                            quads_direct.append((a,b,c,d))\n",
    "                    continue\n",
    "\n",
    "                # 其它：按空白切分\n",
    "                toks = [t for t in re.split(r\"\\s+\", line) if t]\n",
    "\n",
    "                # 格式2：4元组\n",
    "                if len(toks) >= 4 and len(toks) % 2 == 0 and len(toks) % 4 == 0:\n",
    "                    # 每4个为一组\n",
    "                    for i in range(0, len(toks), 4):\n",
    "                        quads_direct.append((toks[i], toks[i+1], toks[i+2], toks[i+3]))\n",
    "                    continue\n",
    "\n",
    "                # 格式1：两列成对（最常见）\n",
    "                if len(toks) >= 2:\n",
    "                    a, b = toks[0], toks[1]\n",
    "                    pairs.append((a, b))\n",
    "                    continue\n",
    "                # 其他异常行：忽略\n",
    "\n",
    "        items = []\n",
    "        if pairs:\n",
    "            # 可选：对数过多时下采样，避免笛卡尔积过大\n",
    "            if (max_pairs_per_file is not None) and (len(pairs) > max_pairs_per_file):\n",
    "                idx = rng.choice(len(pairs), size=max_pairs_per_file, replace=False)\n",
    "                pairs = [pairs[i] for i in sorted(idx)]\n",
    "\n",
    "            # 生成笛卡尔积（避免同一对与自身配对）\n",
    "            for i, (a,b) in enumerate(pairs):\n",
    "                for j, (c,d) in enumerate(pairs):\n",
    "                    if i == j: \n",
    "                        continue\n",
    "                    items.append((a,b,c,d))\n",
    "\n",
    "        # 合并直接的 4 元组\n",
    "        if quads_direct:\n",
    "            items.extend(quads_direct)\n",
    "\n",
    "        if items:\n",
    "            tasks.append((name, items))\n",
    "\n",
    "    return tasks if tasks else None\n",
    "\n",
    "def eval_bats(wv, mapping, tasks, case_insensitive=True, verbose=False):\n",
    "    \"\"\"\n",
    "    评测 BATS：返回 (micro, macro, per_task, total_used)\n",
    "      - micro: 所有题目联合准确率\n",
    "      - macro: 各任务准确率的平均\n",
    "      - per_task: [(task, acc, used, oov), ...]\n",
    "    \"\"\"\n",
    "    def get_vec_ci(word):\n",
    "        v = get_vec(wv, mapping, word, case_insensitive)\n",
    "        if v is None:\n",
    "            return None\n",
    "        return v / (np.linalg.norm(v) + 1e-12)\n",
    "\n",
    "    vocab_mat = l2_normalize(wv.vectors)\n",
    "    all_t = all_c = 0\n",
    "    per_task = []\n",
    "\n",
    "    for name, items in tasks:\n",
    "        t = c = 0\n",
    "        oov = 0\n",
    "        for a,b,c_,d in items:\n",
    "            va, vb, vc, vd = map(get_vec_ci, (a,b,c_,d))\n",
    "            if va is None or vb is None or vc is None or vd is None:\n",
    "                oov += 1\n",
    "                continue\n",
    "            target = vb - va + vc\n",
    "            target /= (np.linalg.norm(target) + 1e-12)\n",
    "            scores = vocab_mat @ target\n",
    "            # 屏蔽 a,b,c\n",
    "            for w in (a,b,c_):\n",
    "                key = (mapping[w.lower()] if (case_insensitive and w.lower() in mapping) else w)\n",
    "                idx = wv.key_to_index.get(key, None)\n",
    "                if idx is not None:\n",
    "                    scores[idx] = -np.inf\n",
    "            pred = int(np.argmax(scores))\n",
    "            didx = wv.key_to_index.get((mapping[d.lower()] if (case_insensitive and d.lower() in mapping) else d), None)\n",
    "            if didx is not None:\n",
    "                t += 1\n",
    "                if pred == didx:\n",
    "                    c += 1\n",
    "        acc = (c / t) if t > 0 else None\n",
    "        per_task.append((name, acc, t, oov))\n",
    "        all_t += t; all_c += c\n",
    "\n",
    "    micro = (all_c / all_t) if all_t > 0 else None\n",
    "    valid = [acc for _, acc, used, _ in per_task if (acc is not None and used > 0)]\n",
    "    macro = (sum(valid) / len(valid)) if valid else None\n",
    "\n",
    "    if verbose:\n",
    "        for name, acc, used, oov in per_task[:5]:\n",
    "            print(f\"  {name:40s} acc={('%.2f%%'%(acc*100)) if acc is not None else 'None':>7s}  used={used:5d}  oov={oov:5d}\")\n",
    "    return micro, macro, per_task, all_t\n",
    "\n",
    "# ----------------- 主流程：自动发现模型并评测 -----------------\n",
    "def evaluate_models(vec_paths=None, data_dir=\"data\", case_insensitive=True, save_json=\"outputs/bench_results.json\"):\n",
    "    data_dir = Path(data_dir)\n",
    "    # 自动发现模型\n",
    "    if vec_paths is None:\n",
    "        vec_paths = sorted([str(p) for p in Path(\"outputs\").glob(\"*.vec\")])\n",
    "    if not vec_paths:\n",
    "        print(\"未找到 outputs/*.vec，请先训练或指定 vec_paths。\"); return\n",
    "\n",
    "    # 尝试加载各数据集\n",
    "    simlex = (load_simlex(data_dir/\"SimLex-999.txt\") or load_simlex(data_dir/\"SimLex-999.csv\"))\n",
    "    ws353 = (load_wordsim(data_dir/\"wordsim353.csv\") or load_wordsim(data_dir/\"wordsim_similarity_goldstandard.txt\"))\n",
    "    goog = load_google_analogies(data_dir/\"questions-words.txt\")\n",
    "    bats = load_bats(data_dir/\"BATS_3.0\")\n",
    "\n",
    "    print(\"Datasets found:\",\n",
    "          f\"SimLex={'Y' if simlex else 'N'}\",\n",
    "          f\"WS353={'Y' if ws353 else 'N'}\",\n",
    "          f\"Google={'Y' if goog else 'N'}\",\n",
    "          f\"BATS={'Y' if bats else 'N'}\")\n",
    "\n",
    "    all_results = []\n",
    "    for vec in vec_paths:\n",
    "        print(f\"\\n=== Evaluating: {Path(vec).name} ===\")\n",
    "        wv, mapping = load_vectors(vec, lowercase=case_insensitive)\n",
    "\n",
    "        model_res = {\"model\": Path(vec).name}\n",
    "\n",
    "        # SimLex-999\n",
    "        if simlex:\n",
    "            pairs, gold = simlex\n",
    "            rho, cov = eval_similarity(wv, mapping, pairs, gold, case_insensitive)\n",
    "            print(f\"SimLex-999 Spearman: {None if rho is None else round(rho,4)} | coverage={cov:.2%}\")\n",
    "            model_res[\"simlex_rho\"] = rho; model_res[\"simlex_cov\"] = cov\n",
    "\n",
    "        # WordSim-353\n",
    "        if ws353:\n",
    "            pairs, gold = ws353\n",
    "            rho, cov = eval_similarity(wv, mapping, pairs, gold, case_insensitive)\n",
    "            print(f\"WordSim-353 Spearman: {None if rho is None else round(rho,4)} | coverage={cov:.2%}\")\n",
    "            model_res[\"ws353_rho\"] = rho; model_res[\"ws353_cov\"] = cov\n",
    "\n",
    "        # Google analogies\n",
    "        if goog:\n",
    "            overall, per_sec, total = eval_analogy(wv, mapping, goog, case_insensitive)\n",
    "            acc_str = f\"{overall*100:.2f}%\" if overall is not None else \"None\"\n",
    "            print(f\"Google Analogies Acc: {acc_str} | used={total}\")\n",
    "            # 可选：打印前几类\n",
    "            for name, acc, t in per_sec[:5]:\n",
    "                if acc is not None:\n",
    "                    print(f\"  - {name:20s} {acc*100:6.2f}% (n={t})\")\n",
    "            model_res[\"google_acc\"] = overall; model_res[\"google_used\"] = total\n",
    "\n",
    "        # BATS\n",
    "        if bats:\n",
    "            micro, macro, per_task, total = eval_bats(wv, mapping, bats, case_insensitive)\n",
    "            mi_str = f\"{micro*100:.2f}%\" if micro is not None else \"None\"\n",
    "            ma_str = f\"{macro*100:.2f}%\" if macro is not None else \"None\"\n",
    "            print(f\"BATS micro: {mi_str} | macro: {ma_str} | used={total}\")\n",
    "            model_res[\"bats_micro\"] = micro; model_res[\"bats_macro\"] = macro; model_res[\"bats_used\"] = total\n",
    "\n",
    "        all_results.append(model_res)\n",
    "\n",
    "    # 保存结果 JSON 方便后续对比/作图\n",
    "    os.makedirs(\"outputs\", exist_ok=True)\n",
    "    with open(save_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_results, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"\\n✅ Done. Results saved to {save_json}\")\n",
    "    return all_results\n",
    "\n",
    "# ----------------- 直接运行 -----------------\n",
    "if __name__ == \"__main__\":\n",
    "    evaluate_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25cb34e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved → outputs/alpha_benchmarks_all.png\n",
      "✅ Saved → outputs/alpha_simlex.png\n",
      "✅ Saved → outputs/alpha_ws353.png\n",
      "✅ Saved → outputs/alpha_google.png\n",
      "✅ Saved → outputs/alpha_bats_micro.png\n",
      "✅ Saved → outputs/alpha_bats_macro.png\n"
     ]
    }
   ],
   "source": [
    "# plot_benchmarks_alpha_all_and_separate.py\n",
    "import json, re\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# ------- 读取结果 -------\n",
    "with open(\"outputs/bench_results.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    rows = json.load(f)\n",
    "\n",
    "def parse_alpha(name: str):\n",
    "    stem = Path(name).stem  # 去扩展名，避免 \"0.500.\"\n",
    "    m = re.search(r'alpha([0-9]+(?:\\.[0-9]+)?)', stem)\n",
    "    return float(m.group(1)) if m else None\n",
    "\n",
    "pts = [(parse_alpha(r[\"model\"]), r) for r in rows if parse_alpha(r[\"model\"]) is not None]\n",
    "pts.sort(key=lambda x: x[0])\n",
    "if not pts:\n",
    "    raise SystemExit(\"No alpha-tagged models in outputs/bench_results.json\")\n",
    "\n",
    "alph = [a for a,_ in pts]\n",
    "simlex = [r.get(\"simlex_rho\") for _,r in pts]\n",
    "ws353  = [r.get(\"ws353_rho\")  for _,r in pts]\n",
    "goog   = [r.get(\"google_acc\") for _,r in pts]\n",
    "bats_mi = [r.get(\"bats_micro\") for _,r in pts]\n",
    "bats_ma = [r.get(\"bats_macro\") for _,r in pts]\n",
    "\n",
    "def add_line(xs_all, ys_all, label):\n",
    "    xs = [x for x,y in zip(xs_all, ys_all) if y is not None]\n",
    "    ys = [y for y in ys_all if y is not None]\n",
    "    if xs and ys:\n",
    "        plt.plot(xs, ys, marker='o', markersize=3, linewidth=1, label=label)\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "Path(\"outputs\").mkdir(exist_ok=True)\n",
    "\n",
    "# ------- 合并一张图（一起画） -------\n",
    "plt.figure(figsize=(10,6))\n",
    "any_plotted = False\n",
    "any_plotted |= add_line(alph, simlex, \"SimLex-999 ρ (↑)\")\n",
    "any_plotted |= add_line(alph, ws353,  \"WordSim-353 ρ (↑)\")\n",
    "any_plotted |= add_line(alph, goog,   \"Google analogies acc (↑)\")\n",
    "any_plotted |= add_line(alph, bats_mi,\"BATS micro acc (↑)\")\n",
    "any_plotted |= add_line(alph, bats_ma,\"BATS macro acc (↑)\")\n",
    "plt.xlabel(\"ns_exponent α\")\n",
    "plt.ylabel(\"score\")\n",
    "plt.title(\"Benchmarks vs α (combined)\")\n",
    "plt.grid(alpha=0.3)\n",
    "if any_plotted:\n",
    "    plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"outputs/alpha_benchmarks_all.png\", dpi=200)\n",
    "plt.close()\n",
    "print(\"✅ Saved → outputs/alpha_benchmarks_all.png\")\n",
    "\n",
    "# ------- 分别单画（五张图，自动跳过缺失） -------\n",
    "def plot_one(xs_all, ys_all, title, ylabel, out_path):\n",
    "    xs = [x for x,y in zip(xs_all, ys_all) if y is not None]\n",
    "    ys = [y for y in ys_all if y is not None]\n",
    "    if not xs:\n",
    "        print(f\"⚠️ {title}: 无可用数据，跳过。\"); return\n",
    "    plt.figure(figsize=(9,5))\n",
    "    plt.plot(xs, ys, marker='o', markersize=3, linewidth=1)\n",
    "    plt.xlabel(\"ns_exponent α\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=200)\n",
    "    plt.close()\n",
    "    print(f\"✅ Saved → {out_path}\")\n",
    "\n",
    "plot_one(alph, simlex, \"SimLex-999 vs α\",        \"Spearman ρ (↑)\",   \"outputs/alpha_simlex.png\")\n",
    "plot_one(alph, ws353,  \"WordSim-353 vs α\",       \"Spearman ρ (↑)\",   \"outputs/alpha_ws353.png\")\n",
    "plot_one(alph, goog,   \"Google Analogies vs α\",  \"Accuracy (↑)\",     \"outputs/alpha_google.png\")\n",
    "plot_one(alph, bats_mi,\"BATS Micro vs α\",        \"Accuracy (↑)\",     \"outputs/alpha_bats_micro.png\")\n",
    "plot_one(alph, bats_ma,\"BATS Macro vs α\",        \"Accuracy (↑)\",     \"outputs/alpha_bats_macro.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "w2v_sgns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
