{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "321692e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Load] outputs/w2v_text8_sgns.vec\n",
      "Vocab=71290, dim=300\n",
      "\n",
      "[Intrinsic] kNN Coherence …\n",
      "  coherence@10 = 0.6936  (越高越紧密)\n",
      "\n",
      "[Intrinsic] Hubness …\n",
      "  hubness skew=2.1793  gini=0.6105  (越低越好)\n",
      "\n",
      "[Intrinsic] Isotropy …\n",
      "  isotropy score = 0.3758  (越接近 1 越好)\n"
     ]
    }
   ],
   "source": [
    "# eval_suite.py\n",
    "import os, sys, math, json, random\n",
    "import numpy as np\n",
    "\n",
    "# ---------- 尝试用 gensim 读取 .vec；失败则手写解析 ----------\n",
    "def load_vectors(vec_path):\n",
    "    try:\n",
    "        from gensim.models import KeyedVectors\n",
    "        wv = KeyedVectors.load_word2vec_format(vec_path, binary=False)\n",
    "        vocab = wv.index_to_key\n",
    "        W = np.stack([wv[w] for w in vocab], axis=0)\n",
    "        return vocab, W\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] gensim 读取失败，改用手写解析:\", e)\n",
    "        vocab, vecs = [], []\n",
    "        with open(vec_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            first = f.readline().strip().split()\n",
    "            # 如果第一行是 \"V D\" 头\n",
    "            if len(first) == 2 and all(s.isdigit() for s in first):\n",
    "                pass\n",
    "            else:\n",
    "                # 第一行就是一个词向量\n",
    "                parts = first\n",
    "                vocab.append(parts[0]); vecs.append([float(x) for x in parts[1:]])\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) < 3: continue\n",
    "                vocab.append(parts[0]); vecs.append([float(x) for x in parts[1:]])\n",
    "        W = np.array(vecs, dtype=np.float32)\n",
    "        return vocab, W\n",
    "\n",
    "def l2_normalize(mat, eps=1e-9):\n",
    "    nrm = np.linalg.norm(mat, axis=1, keepdims=True) + eps\n",
    "    return mat / nrm\n",
    "\n",
    "def cosine_matrix(A, B):\n",
    "    # A,B 均需先单位化\n",
    "    return A @ B.T\n",
    "\n",
    "# ---------- Intrinsic 1: kNN Coherence ----------\n",
    "def knn_coherence(W, sample_size=1000, k=10, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    V = W.shape[0]\n",
    "    idx = rng.choice(V, size=min(sample_size, V), replace=False)\n",
    "    X = l2_normalize(W[idx])\n",
    "    S = cosine_matrix(X, X)\n",
    "    # 排除自身：将对角设为 -inf 以便不被选入 top-k\n",
    "    np.fill_diagonal(S, -np.inf)\n",
    "    # top-k 平均相似度\n",
    "    part = np.partition(S, -k, axis=1)[:, -k:]       # [n, k] 未排序的 top-k 值\n",
    "    coh = part.mean()\n",
    "    return float(coh)\n",
    "\n",
    "# ---------- Intrinsic 2: Hubness ----------\n",
    "def hubness(W, sample_size=2000, k=10, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    V = W.shape[0]\n",
    "    idx = rng.choice(V, size=min(sample_size, V), replace=False)\n",
    "    X = l2_normalize(W[idx])\n",
    "    S = cosine_matrix(X, X)\n",
    "    np.fill_diagonal(S, -np.inf)\n",
    "    # 统计每个点被其他点选为top-k邻居的次数\n",
    "    topk_idx = np.argpartition(S, -k, axis=1)[:, -k:]  # [n, k]\n",
    "    counts = np.bincount(topk_idx.reshape(-1), minlength=X.shape[0]).astype(np.float64)\n",
    "    # 归一化 + 偏度（第三中心矩/标准差^3）\n",
    "    c = counts / counts.sum()\n",
    "    mu = c.mean()\n",
    "    sd = c.std() + 1e-12\n",
    "    skew = (((c - mu)**3).mean()) / (sd**3)\n",
    "    # Gini（可选）：越接近0越均匀\n",
    "    sorted_c = np.sort(c)\n",
    "    n = len(sorted_c)\n",
    "    gini = 1 - 2 * np.sum((n - np.arange(1, n+1) + 0.5) * sorted_c) / (n * sorted_c.sum() + 1e-12)\n",
    "    return {\"skew\": float(skew), \"gini\": float(gini)}\n",
    "\n",
    "# ---------- Intrinsic 3: Isotropy ----------\n",
    "def isotropy(W):\n",
    "    X = l2_normalize(W)\n",
    "    m = X.mean(axis=0)\n",
    "    iso = 1.0 - float(np.linalg.norm(m))\n",
    "    return max(0.0, min(1.0, iso))  # 裁剪到 [0,1]\n",
    "\n",
    "# ---------- Similarity (Spearman, 无 SciPy 版) ----------\n",
    "def _rankdata(a):\n",
    "    # 稳健的秩（考虑并列：取平均秩）\n",
    "    temp = np.argsort(a)\n",
    "    ranks = np.empty_like(temp, dtype=np.float64)\n",
    "    ranks[temp] = np.arange(len(a))\n",
    "    # 处理 ties\n",
    "    unique, inv, counts = np.unique(a, return_inverse=True, return_counts=True)\n",
    "    cum = np.cumsum(counts)\n",
    "    start = cum - counts\n",
    "    avg = (start + cum - 1) / 2.0\n",
    "    return avg[inv]\n",
    "\n",
    "def spearmanr_no_scipy(x, y):\n",
    "    rx, ry = _rankdata(x), _rankdata(y)\n",
    "    # 皮尔逊相关 on ranks\n",
    "    rxm, rym = rx - rx.mean(), ry - ry.mean()\n",
    "    denom = (np.linalg.norm(rxm) * np.linalg.norm(rym) + 1e-12)\n",
    "    return float(np.dot(rxm, rym) / denom)\n",
    "\n",
    "def evaluate_similarity(vec_path, sim_path):\n",
    "    # 期望 TSV/CSV: word1, word2, score（带表头也行）\n",
    "    vocab, W = load_vectors(vec_path)\n",
    "    word2id = {w:i for i,w in enumerate(vocab)}\n",
    "    pairs, gold = [], []\n",
    "    with open(sim_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if not line.strip(): continue\n",
    "            parts = [p.strip() for p in line.replace(\",\", \"\\t\").split()]\n",
    "            if len(parts) < 3: continue\n",
    "            a, b, s = parts[0], parts[1], parts[2]\n",
    "            if a in word2id and b in word2id:\n",
    "                pairs.append((word2id[a], word2id[b]))\n",
    "                gold.append(float(s))\n",
    "    if not pairs:\n",
    "        return {\"spearman\": None, \"used\": 0, \"total\": 0}\n",
    "    X = l2_normalize(W)\n",
    "    sim = [float(np.dot(X[i], X[j])) for i,j in pairs]\n",
    "    rho = spearmanr_no_scipy(np.array(sim), np.array(gold))\n",
    "    return {\"spearman\": rho, \"used\": len(pairs)}\n",
    "\n",
    "# ---------- Analogy (Google questions-words.txt 格式) ----------\n",
    "def evaluate_analogy(vec_path, qwords_path, case_insensitive=True):\n",
    "    vocab, W = load_vectors(vec_path)\n",
    "    word2id = {w:i for i,w in enumerate(vocab)}\n",
    "    X = l2_normalize(W)\n",
    "    total = correct = 0\n",
    "    with open(qwords_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\":\"):  # 类别头\n",
    "                continue\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) != 4: \n",
    "                continue\n",
    "            a,b,c,d = parts\n",
    "            if case_insensitive:\n",
    "                a,b,c,d = a.lower(), b.lower(), c.lower(), d.lower()\n",
    "            if not all(w in word2id for w in (a,b,c,d)):\n",
    "                continue\n",
    "            va, vb, vc = X[word2id[a]], X[word2id[b]], X[word2id[c]]\n",
    "            target = vb - va + vc\n",
    "            target /= (np.linalg.norm(target) + 1e-9)\n",
    "            scores = X @ target\n",
    "            # 屏蔽 a,b,c 本身\n",
    "            for w in (a,b,c):\n",
    "                scores[word2id[w]] = -np.inf\n",
    "            pred = np.argmax(scores)\n",
    "            total += 1\n",
    "            if pred == word2id[d]:\n",
    "                correct += 1\n",
    "    acc = (correct / total) if total > 0 else None\n",
    "    return {\"accuracy\": acc, \"used\": total}\n",
    "\n",
    "# ---------- CLI ----------\n",
    "def main():\n",
    "    import argparse\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--vec\", type=str, default=\"outputs/w2v_text8_sgns.vec\")\n",
    "    ap.add_argument(\"--similarity\", type=str, default=\"\", help=\"词相似度数据集路径（csv/tsv: w1,w2,score）\")\n",
    "    ap.add_argument(\"--analogy\", type=str, default=\"\", help=\"Google questions-words.txt 路径\")\n",
    "    ap.add_argument(\"--sample-size\", type=int, default=1000)\n",
    "    ap.add_argument(\"--knn\", type=int, default=10)\n",
    "    args, _ = ap.parse_known_args()\n",
    "\n",
    "    print(f\"[Load] {args.vec}\")\n",
    "    vocab, W = load_vectors(args.vec)\n",
    "    print(f\"Vocab={len(vocab)}, dim={W.shape[1]}\")\n",
    "\n",
    "    print(\"\\n[Intrinsic] kNN Coherence …\")\n",
    "    coh = knn_coherence(W, sample_size=args.sample_size, k=args.knn)\n",
    "    print(f\"  coherence@{args.knn} = {coh:.4f}  (越高越紧密)\")\n",
    "\n",
    "    print(\"\\n[Intrinsic] Hubness …\")\n",
    "    hub = hubness(W, sample_size=min(2000, args.sample_size*2), k=args.knn)\n",
    "    print(f\"  hubness skew={hub['skew']:.4f}  gini={hub['gini']:.4f}  (越低越好)\")\n",
    "\n",
    "    print(\"\\n[Intrinsic] Isotropy …\")\n",
    "    iso = isotropy(W)\n",
    "    print(f\"  isotropy score = {iso:.4f}  (越接近 1 越好)\")\n",
    "\n",
    "    if args.similarity:\n",
    "        print(\"\\n[Similarity] Spearman ρ …\")\n",
    "        sim = evaluate_similarity(args.vec, args.similarity)\n",
    "        if sim[\"spearman\"] is None:\n",
    "            print(\"  无有效样本（可能是 OOV 太多）\")\n",
    "        else:\n",
    "            print(f\"  spearman = {sim['spearman']:.4f}  (越高越好)  | used={sim['used']}\")\n",
    "\n",
    "    if args.analogy:\n",
    "        print(\"\\n[Analogy] 3CosAdd Accuracy …\")\n",
    "        ana = evaluate_analogy(args.vec, args.analogy)\n",
    "        if ana[\"accuracy\"] is None:\n",
    "            print(\"  无有效样本（可能是 OOV 太多）\")\n",
    "        else:\n",
    "            print(f\"  accuracy = {ana['accuracy']*100:.2f}%  | used={ana['used']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b9f74b",
   "metadata": {},
   "source": [
    "# 复评（ABTT）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03aa2dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ABTT done: r=2\n",
      "→ outputs/w2v_text8_sgns_abtt.vec\n"
     ]
    }
   ],
   "source": [
    "# postprocess_abtt.py\n",
    "import numpy as np, os\n",
    "from pathlib import Path\n",
    "\n",
    "def load_vec(path):\n",
    "    vocab, vecs = [], []\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        head = f.readline().strip().split()\n",
    "        if len(head)==2 and all(s.isdigit() for s in head):\n",
    "            pass\n",
    "        else:\n",
    "            parts = head; vocab.append(parts[0]); vecs.append([float(x) for x in parts[1:]])\n",
    "        for line in f:\n",
    "            ps = line.strip().split()\n",
    "            if len(ps)<3: continue\n",
    "            vocab.append(ps[0]); vecs.append([float(x) for x in ps[1:]])\n",
    "    W = np.asarray(vecs, dtype=np.float32)\n",
    "    return vocab, W\n",
    "\n",
    "def save_vec(path, vocab, W):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        print(len(vocab), W.shape[1], file=f)\n",
    "        for w, v in zip(vocab, W):\n",
    "            f.write(w + \" \" + \" \".join(f\"{x:.6f}\" for x in v) + \"\\n\")\n",
    "\n",
    "def l2norm(W, eps=1e-9):\n",
    "    n = np.linalg.norm(W, axis=1, keepdims=True) + eps\n",
    "    return W / n\n",
    "\n",
    "def abtt(W, r=2):\n",
    "    # 单位化 → 去均值 → SVD → 去前r主成分 → 再单位化\n",
    "    X = l2norm(W)\n",
    "    mu = X.mean(axis=0, keepdims=True)\n",
    "    Xc = X - mu\n",
    "    U, S, Vt = np.linalg.svd(Xc, full_matrices=False)\n",
    "    P = Vt[:r].T   # [D,r]\n",
    "    X_hat = Xc - (Xc @ P) @ P.T\n",
    "    return l2norm(X_hat)\n",
    "\n",
    "def main(in_path=\"outputs/w2v_text8_sgns.vec\", out_path=\"outputs/w2v_text8_sgns_abtt.vec\", r=2):\n",
    "    vocab, W = load_vec(in_path)\n",
    "    W2 = abtt(W, r=r)\n",
    "    os.makedirs(\"outputs\", exist_ok=True)\n",
    "    save_vec(out_path, vocab, W2)\n",
    "    print(f\"✅ ABTT done: r={r}\\n→ {out_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 直接运行：python postprocess_abtt.py\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05e386d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Load] outputs/w2v_text8_sgns_abtt.vec\n",
      "Vocab=71290, dim=300\n",
      "\n",
      "[Intrinsic] kNN Coherence …\n",
      "  coherence@10 = 0.3983  (越高越紧密)\n",
      "\n",
      "[Intrinsic] Hubness …\n",
      "  hubness skew=1.4041  gini=0.4672  (越低越好)\n",
      "\n",
      "[Intrinsic] Isotropy …\n",
      "  isotropy score = 0.9673  (越接近 1 越好)\n"
     ]
    }
   ],
   "source": [
    "# eval_suite.py\n",
    "import os, sys, math, json, random\n",
    "import numpy as np\n",
    "\n",
    "# ---------- 尝试用 gensim 读取 .vec；失败则手写解析 ----------\n",
    "def load_vectors(vec_path):\n",
    "    try:\n",
    "        from gensim.models import KeyedVectors\n",
    "        wv = KeyedVectors.load_word2vec_format(vec_path, binary=False)\n",
    "        vocab = wv.index_to_key\n",
    "        W = np.stack([wv[w] for w in vocab], axis=0)\n",
    "        return vocab, W\n",
    "    except Exception as e:\n",
    "        print(\"[WARN] gensim 读取失败，改用手写解析:\", e)\n",
    "        vocab, vecs = [], []\n",
    "        with open(vec_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            first = f.readline().strip().split()\n",
    "            # 如果第一行是 \"V D\" 头\n",
    "            if len(first) == 2 and all(s.isdigit() for s in first):\n",
    "                pass\n",
    "            else:\n",
    "                # 第一行就是一个词向量\n",
    "                parts = first\n",
    "                vocab.append(parts[0]); vecs.append([float(x) for x in parts[1:]])\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) < 3: continue\n",
    "                vocab.append(parts[0]); vecs.append([float(x) for x in parts[1:]])\n",
    "        W = np.array(vecs, dtype=np.float32)\n",
    "        return vocab, W\n",
    "\n",
    "def l2_normalize(mat, eps=1e-9):\n",
    "    nrm = np.linalg.norm(mat, axis=1, keepdims=True) + eps\n",
    "    return mat / nrm\n",
    "\n",
    "def cosine_matrix(A, B):\n",
    "    # A,B 均需先单位化\n",
    "    return A @ B.T\n",
    "\n",
    "# ---------- Intrinsic 1: kNN Coherence ----------\n",
    "def knn_coherence(W, sample_size=1000, k=10, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    V = W.shape[0]\n",
    "    idx = rng.choice(V, size=min(sample_size, V), replace=False)\n",
    "    X = l2_normalize(W[idx])\n",
    "    S = cosine_matrix(X, X)\n",
    "    # 排除自身：将对角设为 -inf 以便不被选入 top-k\n",
    "    np.fill_diagonal(S, -np.inf)\n",
    "    # top-k 平均相似度\n",
    "    part = np.partition(S, -k, axis=1)[:, -k:]       # [n, k] 未排序的 top-k 值\n",
    "    coh = part.mean()\n",
    "    return float(coh)\n",
    "\n",
    "# ---------- Intrinsic 2: Hubness ----------\n",
    "def hubness(W, sample_size=2000, k=10, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    V = W.shape[0]\n",
    "    idx = rng.choice(V, size=min(sample_size, V), replace=False)\n",
    "    X = l2_normalize(W[idx])\n",
    "    S = cosine_matrix(X, X)\n",
    "    np.fill_diagonal(S, -np.inf)\n",
    "    # 统计每个点被其他点选为top-k邻居的次数\n",
    "    topk_idx = np.argpartition(S, -k, axis=1)[:, -k:]  # [n, k]\n",
    "    counts = np.bincount(topk_idx.reshape(-1), minlength=X.shape[0]).astype(np.float64)\n",
    "    # 归一化 + 偏度（第三中心矩/标准差^3）\n",
    "    c = counts / counts.sum()\n",
    "    mu = c.mean()\n",
    "    sd = c.std() + 1e-12\n",
    "    skew = (((c - mu)**3).mean()) / (sd**3)\n",
    "    # Gini（可选）：越接近0越均匀\n",
    "    sorted_c = np.sort(c)\n",
    "    n = len(sorted_c)\n",
    "    gini = 1 - 2 * np.sum((n - np.arange(1, n+1) + 0.5) * sorted_c) / (n * sorted_c.sum() + 1e-12)\n",
    "    return {\"skew\": float(skew), \"gini\": float(gini)}\n",
    "\n",
    "# ---------- Intrinsic 3: Isotropy ----------\n",
    "def isotropy(W):\n",
    "    X = l2_normalize(W)\n",
    "    m = X.mean(axis=0)\n",
    "    iso = 1.0 - float(np.linalg.norm(m))\n",
    "    return max(0.0, min(1.0, iso))  # 裁剪到 [0,1]\n",
    "\n",
    "# ---------- Similarity (Spearman, 无 SciPy 版) ----------\n",
    "def _rankdata(a):\n",
    "    # 稳健的秩（考虑并列：取平均秩）\n",
    "    temp = np.argsort(a)\n",
    "    ranks = np.empty_like(temp, dtype=np.float64)\n",
    "    ranks[temp] = np.arange(len(a))\n",
    "    # 处理 ties\n",
    "    unique, inv, counts = np.unique(a, return_inverse=True, return_counts=True)\n",
    "    cum = np.cumsum(counts)\n",
    "    start = cum - counts\n",
    "    avg = (start + cum - 1) / 2.0\n",
    "    return avg[inv]\n",
    "\n",
    "def spearmanr_no_scipy(x, y):\n",
    "    rx, ry = _rankdata(x), _rankdata(y)\n",
    "    # 皮尔逊相关 on ranks\n",
    "    rxm, rym = rx - rx.mean(), ry - ry.mean()\n",
    "    denom = (np.linalg.norm(rxm) * np.linalg.norm(rym) + 1e-12)\n",
    "    return float(np.dot(rxm, rym) / denom)\n",
    "\n",
    "def evaluate_similarity(vec_path, sim_path):\n",
    "    # 期望 TSV/CSV: word1, word2, score（带表头也行）\n",
    "    vocab, W = load_vectors(vec_path)\n",
    "    word2id = {w:i for i,w in enumerate(vocab)}\n",
    "    pairs, gold = [], []\n",
    "    with open(sim_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if not line.strip(): continue\n",
    "            parts = [p.strip() for p in line.replace(\",\", \"\\t\").split()]\n",
    "            if len(parts) < 3: continue\n",
    "            a, b, s = parts[0], parts[1], parts[2]\n",
    "            if a in word2id and b in word2id:\n",
    "                pairs.append((word2id[a], word2id[b]))\n",
    "                gold.append(float(s))\n",
    "    if not pairs:\n",
    "        return {\"spearman\": None, \"used\": 0, \"total\": 0}\n",
    "    X = l2_normalize(W)\n",
    "    sim = [float(np.dot(X[i], X[j])) for i,j in pairs]\n",
    "    rho = spearmanr_no_scipy(np.array(sim), np.array(gold))\n",
    "    return {\"spearman\": rho, \"used\": len(pairs)}\n",
    "\n",
    "# ---------- Analogy (Google questions-words.txt 格式) ----------\n",
    "def evaluate_analogy(vec_path, qwords_path, case_insensitive=True):\n",
    "    vocab, W = load_vectors(vec_path)\n",
    "    word2id = {w:i for i,w in enumerate(vocab)}\n",
    "    X = l2_normalize(W)\n",
    "    total = correct = 0\n",
    "    with open(qwords_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\":\"):  # 类别头\n",
    "                continue\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) != 4: \n",
    "                continue\n",
    "            a,b,c,d = parts\n",
    "            if case_insensitive:\n",
    "                a,b,c,d = a.lower(), b.lower(), c.lower(), d.lower()\n",
    "            if not all(w in word2id for w in (a,b,c,d)):\n",
    "                continue\n",
    "            va, vb, vc = X[word2id[a]], X[word2id[b]], X[word2id[c]]\n",
    "            target = vb - va + vc\n",
    "            target /= (np.linalg.norm(target) + 1e-9)\n",
    "            scores = X @ target\n",
    "            # 屏蔽 a,b,c 本身\n",
    "            for w in (a,b,c):\n",
    "                scores[word2id[w]] = -np.inf\n",
    "            pred = np.argmax(scores)\n",
    "            total += 1\n",
    "            if pred == word2id[d]:\n",
    "                correct += 1\n",
    "    acc = (correct / total) if total > 0 else None\n",
    "    return {\"accuracy\": acc, \"used\": total}\n",
    "\n",
    "# ---------- CLI ----------\n",
    "def main():\n",
    "    import argparse\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--vec\", type=str, default=\"outputs/w2v_text8_sgns_abtt.vec\")\n",
    "    ap.add_argument(\"--similarity\", type=str, default=\"\", help=\"词相似度数据集路径（csv/tsv: w1,w2,score）\")\n",
    "    ap.add_argument(\"--analogy\", type=str, default=\"\", help=\"Google questions-words.txt 路径\")\n",
    "    ap.add_argument(\"--sample-size\", type=int, default=1000)\n",
    "    ap.add_argument(\"--knn\", type=int, default=10)\n",
    "    args, _ = ap.parse_known_args()\n",
    "\n",
    "    print(f\"[Load] {args.vec}\")\n",
    "    vocab, W = load_vectors(args.vec)\n",
    "    print(f\"Vocab={len(vocab)}, dim={W.shape[1]}\")\n",
    "\n",
    "    print(\"\\n[Intrinsic] kNN Coherence …\")\n",
    "    coh = knn_coherence(W, sample_size=args.sample_size, k=args.knn)\n",
    "    print(f\"  coherence@{args.knn} = {coh:.4f}  (越高越紧密)\")\n",
    "\n",
    "    print(\"\\n[Intrinsic] Hubness …\")\n",
    "    hub = hubness(W, sample_size=min(2000, args.sample_size*2), k=args.knn)\n",
    "    print(f\"  hubness skew={hub['skew']:.4f}  gini={hub['gini']:.4f}  (越低越好)\")\n",
    "\n",
    "    print(\"\\n[Intrinsic] Isotropy …\")\n",
    "    iso = isotropy(W)\n",
    "    print(f\"  isotropy score = {iso:.4f}  (越接近 1 越好)\")\n",
    "\n",
    "    if args.similarity:\n",
    "        print(\"\\n[Similarity] Spearman ρ …\")\n",
    "        sim = evaluate_similarity(args.vec, args.similarity)\n",
    "        if sim[\"spearman\"] is None:\n",
    "            print(\"  无有效样本（可能是 OOV 太多）\")\n",
    "        else:\n",
    "            print(f\"  spearman = {sim['spearman']:.4f}  (越高越好)  | used={sim['used']}\")\n",
    "\n",
    "    if args.analogy:\n",
    "        print(\"\\n[Analogy] 3CosAdd Accuracy …\")\n",
    "        ana = evaluate_analogy(args.vec, args.analogy)\n",
    "        if ana[\"accuracy\"] is None:\n",
    "            print(\"  无有效样本（可能是 OOV 太多）\")\n",
    "        else:\n",
    "            print(f\"  accuracy = {ana['accuracy']*100:.2f}%  | used={ana['used']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "w2v_sgns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
